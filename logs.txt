* 
* ==> Audit <==
* |--------------|-----------------------------|----------|-----------------------|---------|---------------------|---------------------|
|   Command    |            Args             | Profile  |         User          | Version |     Start Time      |      End Time       |
|--------------|-----------------------------|----------|-----------------------|---------|---------------------|---------------------|
| start        |                             | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 30 Nov 23 19:06 CET | 30 Nov 23 19:42 CET |
| start        |                             | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 01 Dec 23 18:50 CET | 01 Dec 23 18:51 CET |
| stop         |                             | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 01 Dec 23 19:38 CET | 01 Dec 23 19:39 CET |
| start        |                             | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 08 Dec 23 17:52 CET |                     |
| stop         |                             | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 08 Dec 23 17:56 CET |                     |
| start        |                             | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 08 Dec 23 17:56 CET |                     |
| start        |                             | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 08 Dec 23 18:17 CET | 08 Dec 23 18:18 CET |
| stop         |                             | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 08 Dec 23 19:13 CET | 08 Dec 23 19:13 CET |
| start        |                             | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 08 Dec 23 19:39 CET | 08 Dec 23 19:40 CET |
| start        |                             | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 17 Dec 23 12:08 CET | 17 Dec 23 12:09 CET |
| update-check |                             | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 17 Dec 23 12:18 CET | 17 Dec 23 12:18 CET |
| service      | goprom                      | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 17 Dec 23 12:20 CET |                     |
| docker-env   |                             | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 17 Dec 23 12:26 CET | 17 Dec 23 12:26 CET |
| service      | goprom                      | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 17 Dec 23 12:34 CET |                     |
| service      | goprom --url                | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 17 Dec 23 12:34 CET |                     |
| service      | goprom .                    | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 17 Dec 23 12:37 CET |                     |
| service      | goprom                      | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 17 Dec 23 12:41 CET |                     |
| service      | goprom                      | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 17 Dec 23 12:46 CET |                     |
| service      | my-nginx-64666fbb86-cp7xk   | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 17 Dec 23 12:47 CET |                     |
| service      | goprom                      | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 17 Dec 23 12:47 CET |                     |
| service      | goprom                      | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 17 Dec 23 12:50 CET |                     |
| service      | goprom                      | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 17 Dec 23 12:51 CET |                     |
| service      | goprom-metrics              | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 17 Dec 23 12:52 CET |                     |
| start        |                             | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 18 Dec 23 14:53 CET | 18 Dec 23 14:54 CET |
| start        |                             | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 18 Dec 23 15:13 CET | 18 Dec 23 15:15 CET |
| update-check |                             | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 18 Dec 23 15:23 CET | 18 Dec 23 15:23 CET |
| start        |                             | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 18 Dec 23 16:55 CET | 18 Dec 23 16:56 CET |
| service      | devopsprojectmicro-nodeport | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 18 Dec 23 20:12 CET |                     |
| service      | devopsprojectmicro-nodeport | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 18 Dec 23 20:14 CET |                     |
| service      | devopsprojectmicro-nodeport | minikube | DESKTOP-1A692GJ\Houda | v1.32.0 | 18 Dec 23 20:16 CET |                     |
|--------------|-----------------------------|----------|-----------------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/12/18 16:55:32
Running on machine: DESKTOP-1A692GJ
Binary: Built with gc go1.21.3 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1218 16:55:32.983958   25572 out.go:296] Setting OutFile to fd 84 ...
I1218 16:55:32.985582   25572 out.go:343] TERM=,COLORTERM=, which probably does not support color
I1218 16:55:32.985582   25572 out.go:309] Setting ErrFile to fd 100...
I1218 16:55:32.985582   25572 out.go:343] TERM=,COLORTERM=, which probably does not support color
W1218 16:55:32.996125   25572 root.go:314] Error reading config file at C:\Users\Houda\.minikube\config\config.json: open C:\Users\Houda\.minikube\config\config.json: The system cannot find the file specified.
I1218 16:55:33.000525   25572 out.go:303] Setting JSON to false
I1218 16:55:33.004558   25572 start.go:128] hostinfo: {"hostname":"DESKTOP-1A692GJ","uptime":25305,"bootTime":1702889627,"procs":347,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.22621.2861 Build 22621.2861","kernelVersion":"10.0.22621.2861 Build 22621.2861","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"56253f40-da23-4b6c-940c-ae023ab1f3b6"}
W1218 16:55:33.005672   25572 start.go:136] gopshost.Virtualization returned error: not implemented yet
I1218 16:55:33.008068   25572 out.go:177] * minikube v1.32.0 on Microsoft Windows 11 Pro 10.0.22621.2861 Build 22621.2861
I1218 16:55:33.009183   25572 notify.go:220] Checking for updates...
I1218 16:55:33.009688   25572 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1218 16:55:33.009688   25572 driver.go:378] Setting default libvirt URI to qemu:///system
I1218 16:55:33.161550   25572 docker.go:122] docker version: linux-23.0.5:Docker Desktop 4.19.0 (106363)
I1218 16:55:33.164916   25572 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1218 16:55:33.550108   25572 info.go:266] docker info: {ID:d0c60134-83f4-4a0d-b29d-303b43c5f000 Containers:86 ContainersRunning:3 ContainersPaused:0 ContainersStopped:83 Images:18 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:113 OomKillDisable:true NGoroutines:173 SystemTime:2023-12-18 15:55:33.506618954 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:20 MemTotal:8164593664 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:23.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2806fc1057397dbaeefbea0e4e17bddfbd388f38 Expected:2806fc1057397dbaeefbea0e4e17bddfbd388f38} RuncCommit:{ID:v1.1.5-0-gf19387a Expected:v1.1.5-0-gf19387a} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.17.3] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.19] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.4] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:v0.10.0]] Warnings:<nil>}}
I1218 16:55:33.551949   25572 out.go:177] * Using the docker driver based on existing profile
I1218 16:55:33.553788   25572 start.go:298] selected driver: docker
I1218 16:55:33.553788   25572 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Houda:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1218 16:55:33.553788   25572 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1218 16:55:33.560404   25572 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1218 16:55:33.750805   25572 info.go:266] docker info: {ID:d0c60134-83f4-4a0d-b29d-303b43c5f000 Containers:86 ContainersRunning:3 ContainersPaused:0 ContainersStopped:83 Images:18 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:113 OomKillDisable:true NGoroutines:173 SystemTime:2023-12-18 15:55:33.707309815 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:20 MemTotal:8164593664 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:23.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2806fc1057397dbaeefbea0e4e17bddfbd388f38 Expected:2806fc1057397dbaeefbea0e4e17bddfbd388f38} RuncCommit:{ID:v1.1.5-0-gf19387a Expected:v1.1.5-0-gf19387a} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.10.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.17.3] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.19] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.4] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Command line tool for Docker Scout Vendor:Docker Inc. Version:v0.10.0]] Warnings:<nil>}}
I1218 16:55:33.785104   25572 cni.go:84] Creating CNI manager for ""
I1218 16:55:33.785104   25572 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1218 16:55:33.785104   25572 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Houda:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1218 16:55:33.787515   25572 out.go:177] * Starting control plane node minikube in cluster minikube
I1218 16:55:33.788804   25572 cache.go:121] Beginning downloading kic base image for docker with docker
I1218 16:55:33.789879   25572 out.go:177] * Pulling base image ...
I1218 16:55:33.791037   25572 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1218 16:55:33.791037   25572 preload.go:148] Found local preload: C:\Users\Houda\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I1218 16:55:33.791584   25572 cache.go:56] Caching tarball of preloaded images
I1218 16:55:33.791584   25572 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I1218 16:55:33.791689   25572 preload.go:174] Found C:\Users\Houda\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1218 16:55:33.791689   25572 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I1218 16:55:33.791689   25572 profile.go:148] Saving config to C:\Users\Houda\.minikube\profiles\minikube\config.json ...
I1218 16:55:33.920269   25572 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I1218 16:55:33.920269   25572 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I1218 16:55:33.921300   25572 cache.go:194] Successfully downloaded all kic artifacts
I1218 16:55:33.922585   25572 start.go:365] acquiring machines lock for minikube: {Name:mk895a307fd336692cc10567bfa4a3a71959656a Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1218 16:55:33.922585   25572 start.go:369] acquired machines lock for "minikube" in 0s
I1218 16:55:33.923172   25572 start.go:96] Skipping create...Using existing machine configuration
I1218 16:55:33.923172   25572 fix.go:54] fixHost starting: 
I1218 16:55:33.931141   25572 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1218 16:55:34.042956   25572 fix.go:102] recreateIfNeeded on minikube: state=Running err=<nil>
W1218 16:55:34.042956   25572 fix.go:128] unexpected machine state, will restart: <nil>
I1218 16:55:34.044630   25572 out.go:177] * Updating the running docker "minikube" container ...
I1218 16:55:34.046343   25572 machine.go:88] provisioning docker machine ...
I1218 16:55:34.046881   25572 ubuntu.go:169] provisioning hostname "minikube"
I1218 16:55:34.050961   25572 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1218 16:55:34.167997   25572 main.go:141] libmachine: Using SSH client type: native
I1218 16:55:34.170099   25572 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xda47e0] 0xda7320 <nil>  [] 0s} 127.0.0.1 62146 <nil> <nil>}
I1218 16:55:34.170099   25572 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1218 16:55:34.327237   25572 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1218 16:55:34.332711   25572 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1218 16:55:34.445233   25572 main.go:141] libmachine: Using SSH client type: native
I1218 16:55:34.447589   25572 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xda47e0] 0xda7320 <nil>  [] 0s} 127.0.0.1 62146 <nil> <nil>}
I1218 16:55:34.447589   25572 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1218 16:55:34.562435   25572 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1218 16:55:34.562594   25572 ubuntu.go:175] set auth options {CertDir:C:\Users\Houda\.minikube CaCertPath:C:\Users\Houda\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Houda\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Houda\.minikube\machines\server.pem ServerKeyPath:C:\Users\Houda\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Houda\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Houda\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Houda\.minikube}
I1218 16:55:34.562594   25572 ubuntu.go:177] setting up certificates
I1218 16:55:34.563112   25572 provision.go:83] configureAuth start
I1218 16:55:34.567859   25572 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1218 16:55:34.675824   25572 provision.go:138] copyHostCerts
I1218 16:55:34.679090   25572 exec_runner.go:144] found C:\Users\Houda\.minikube/ca.pem, removing ...
I1218 16:55:34.679090   25572 exec_runner.go:203] rm: C:\Users\Houda\.minikube\ca.pem
I1218 16:55:34.679090   25572 exec_runner.go:151] cp: C:\Users\Houda\.minikube\certs\ca.pem --> C:\Users\Houda\.minikube/ca.pem (1074 bytes)
I1218 16:55:34.680217   25572 exec_runner.go:144] found C:\Users\Houda\.minikube/cert.pem, removing ...
I1218 16:55:34.680217   25572 exec_runner.go:203] rm: C:\Users\Houda\.minikube\cert.pem
I1218 16:55:34.680217   25572 exec_runner.go:151] cp: C:\Users\Houda\.minikube\certs\cert.pem --> C:\Users\Houda\.minikube/cert.pem (1119 bytes)
I1218 16:55:34.680754   25572 exec_runner.go:144] found C:\Users\Houda\.minikube/key.pem, removing ...
I1218 16:55:34.680754   25572 exec_runner.go:203] rm: C:\Users\Houda\.minikube\key.pem
I1218 16:55:34.680754   25572 exec_runner.go:151] cp: C:\Users\Houda\.minikube\certs\key.pem --> C:\Users\Houda\.minikube/key.pem (1675 bytes)
I1218 16:55:34.681287   25572 provision.go:112] generating server cert: C:\Users\Houda\.minikube\machines\server.pem ca-key=C:\Users\Houda\.minikube\certs\ca.pem private-key=C:\Users\Houda\.minikube\certs\ca-key.pem org=Houda.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1218 16:55:34.754312   25572 provision.go:172] copyRemoteCerts
I1218 16:55:34.765926   25572 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1218 16:55:34.769495   25572 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1218 16:55:34.877482   25572 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62146 SSHKeyPath:C:\Users\Houda\.minikube\machines\minikube\id_rsa Username:docker}
I1218 16:55:34.966651   25572 ssh_runner.go:362] scp C:\Users\Houda\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1218 16:55:34.988222   25572 ssh_runner.go:362] scp C:\Users\Houda\.minikube\machines\server.pem --> /etc/docker/server.pem (1200 bytes)
I1218 16:55:35.003860   25572 ssh_runner.go:362] scp C:\Users\Houda\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1218 16:55:35.019617   25572 provision.go:86] duration metric: configureAuth took 456.5054ms
I1218 16:55:35.019617   25572 ubuntu.go:193] setting minikube options for container-runtime
I1218 16:55:35.019617   25572 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1218 16:55:35.022886   25572 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1218 16:55:35.126043   25572 main.go:141] libmachine: Using SSH client type: native
I1218 16:55:35.126688   25572 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xda47e0] 0xda7320 <nil>  [] 0s} 127.0.0.1 62146 <nil> <nil>}
I1218 16:55:35.126688   25572 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1218 16:55:35.261348   25572 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1218 16:55:35.261348   25572 ubuntu.go:71] root file system type: overlay
I1218 16:55:35.261888   25572 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1218 16:55:35.265195   25572 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1218 16:55:35.372971   25572 main.go:141] libmachine: Using SSH client type: native
I1218 16:55:35.372971   25572 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xda47e0] 0xda7320 <nil>  [] 0s} 127.0.0.1 62146 <nil> <nil>}
I1218 16:55:35.372971   25572 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1218 16:55:35.498777   25572 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1218 16:55:35.502569   25572 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1218 16:55:35.604801   25572 main.go:141] libmachine: Using SSH client type: native
I1218 16:55:35.605956   25572 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xda47e0] 0xda7320 <nil>  [] 0s} 127.0.0.1 62146 <nil> <nil>}
I1218 16:55:35.605956   25572 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1218 16:55:35.743613   25572 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1218 16:55:35.743613   25572 machine.go:91] provisioned docker machine in 1.6972699s
I1218 16:55:35.744151   25572 start.go:300] post-start starting for "minikube" (driver="docker")
I1218 16:55:35.744151   25572 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1218 16:55:35.756007   25572 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1218 16:55:35.759327   25572 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1218 16:55:35.865135   25572 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62146 SSHKeyPath:C:\Users\Houda\.minikube\machines\minikube\id_rsa Username:docker}
I1218 16:55:35.969855   25572 ssh_runner.go:195] Run: cat /etc/os-release
I1218 16:55:35.974673   25572 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1218 16:55:35.974673   25572 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1218 16:55:35.974673   25572 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1218 16:55:35.974673   25572 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I1218 16:55:35.975214   25572 filesync.go:126] Scanning C:\Users\Houda\.minikube\addons for local assets ...
I1218 16:55:35.975214   25572 filesync.go:126] Scanning C:\Users\Houda\.minikube\files for local assets ...
I1218 16:55:35.975214   25572 start.go:303] post-start completed in 231.063ms
I1218 16:55:35.986829   25572 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1218 16:55:35.990343   25572 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1218 16:55:36.096107   25572 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62146 SSHKeyPath:C:\Users\Houda\.minikube\machines\minikube\id_rsa Username:docker}
I1218 16:55:36.198372   25572 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1218 16:55:36.203997   25572 fix.go:56] fixHost completed within 2.2808248s
I1218 16:55:36.203997   25572 start.go:83] releasing machines lock for "minikube", held for 2.2814118s
I1218 16:55:36.207757   25572 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1218 16:55:36.314459   25572 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1218 16:55:36.318819   25572 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1218 16:55:36.325824   25572 ssh_runner.go:195] Run: cat /version.json
I1218 16:55:36.329211   25572 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1218 16:55:36.435516   25572 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62146 SSHKeyPath:C:\Users\Houda\.minikube\machines\minikube\id_rsa Username:docker}
I1218 16:55:36.446490   25572 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62146 SSHKeyPath:C:\Users\Houda\.minikube\machines\minikube\id_rsa Username:docker}
I1218 16:55:37.045487   25572 ssh_runner.go:195] Run: systemctl --version
I1218 16:55:37.065577   25572 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1218 16:55:37.084392   25572 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W1218 16:55:37.094037   25572 start.go:416] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I1218 16:55:37.106000   25572 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1218 16:55:37.114506   25572 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1218 16:55:37.114506   25572 start.go:472] detecting cgroup driver to use...
I1218 16:55:37.114506   25572 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1218 16:55:37.116200   25572 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1218 16:55:37.139139   25572 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1218 16:55:37.161498   25572 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1218 16:55:37.171195   25572 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I1218 16:55:37.182645   25572 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1218 16:55:37.203404   25572 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1218 16:55:37.224932   25572 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1218 16:55:37.244816   25572 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1218 16:55:37.264967   25572 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1218 16:55:37.284156   25572 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1218 16:55:37.304737   25572 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1218 16:55:37.323836   25572 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1218 16:55:37.344763   25572 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1218 16:55:37.530638   25572 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1218 16:55:48.196038   25572 ssh_runner.go:235] Completed: sudo systemctl restart containerd: (10.6653993s)
I1218 16:55:48.196038   25572 start.go:472] detecting cgroup driver to use...
I1218 16:55:48.196038   25572 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1218 16:55:48.224919   25572 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1218 16:55:48.237283   25572 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1218 16:55:48.262595   25572 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1218 16:55:48.273496   25572 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1218 16:55:48.312333   25572 ssh_runner.go:195] Run: which cri-dockerd
I1218 16:55:48.345028   25572 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1218 16:55:48.354239   25572 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1218 16:55:48.393164   25572 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1218 16:55:48.634253   25572 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1218 16:55:48.756944   25572 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I1218 16:55:48.757449   25572 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1218 16:55:48.781812   25572 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1218 16:55:48.888534   25572 ssh_runner.go:195] Run: sudo systemctl restart docker
I1218 16:55:49.259645   25572 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1218 16:55:49.372525   25572 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1218 16:55:49.512102   25572 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1218 16:55:49.624410   25572 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1218 16:55:49.753100   25572 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1218 16:55:49.784342   25572 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1218 16:55:49.883895   25572 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1218 16:55:49.948764   25572 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1218 16:55:49.975956   25572 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1218 16:55:49.980869   25572 start.go:540] Will wait 60s for crictl version
I1218 16:55:50.009358   25572 ssh_runner.go:195] Run: which crictl
I1218 16:55:50.043754   25572 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1218 16:55:50.125350   25572 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I1218 16:55:50.134804   25572 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1218 16:55:50.216014   25572 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1218 16:55:50.237814   25572 out.go:204] * Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I1218 16:55:50.245938   25572 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1218 16:55:50.453798   25572 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1218 16:55:50.479524   25572 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1218 16:55:50.495208   25572 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1218 16:55:50.608160   25572 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1218 16:55:50.615511   25572 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1218 16:55:50.634431   25572 docker.go:671] Got preloaded images: -- stdout --
quay.io/argoproj/argocd:v2.9.3
nginx:latest
quay.io/argoproj/argocd:v2.9.2
quay.io/prometheus/prometheus:v2.48.0
quay.io/prometheus/node-exporter:v1.7.0
registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1
bitnami/nginx:1.25.3-debian-11-r1
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
quay.io/prometheus/pushgateway:v1.6.2
quay.io/prometheus/alertmanager:v0.26.0
quay.io/prometheus-operator/prometheus-config-reloader:v0.67.0
ghcr.io/dexidp/dex:v2.37.0
redis:7.0.11-alpine
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5
k8s.gcr.io/kube-state-metrics/kube-state-metrics:v1.9.8

-- /stdout --
I1218 16:55:50.635527   25572 docker.go:601] Images already preloaded, skipping extraction
I1218 16:55:50.641014   25572 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1218 16:55:50.658254   25572 docker.go:671] Got preloaded images: -- stdout --
quay.io/argoproj/argocd:v2.9.3
nginx:latest
quay.io/argoproj/argocd:v2.9.2
quay.io/prometheus/prometheus:v2.48.0
quay.io/prometheus/node-exporter:v1.7.0
registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1
bitnami/nginx:1.25.3-debian-11-r1
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
quay.io/prometheus/pushgateway:v1.6.2
quay.io/prometheus/alertmanager:v0.26.0
quay.io/prometheus-operator/prometheus-config-reloader:v0.67.0
ghcr.io/dexidp/dex:v2.37.0
redis:7.0.11-alpine
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5
k8s.gcr.io/kube-state-metrics/kube-state-metrics:v1.9.8

-- /stdout --
I1218 16:55:50.658760   25572 cache_images.go:84] Images are preloaded, skipping loading
I1218 16:55:50.667199   25572 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1218 16:55:50.763807   25572 cni.go:84] Creating CNI manager for ""
I1218 16:55:50.767395   25572 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1218 16:55:50.767910   25572 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1218 16:55:50.767910   25572 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1218 16:55:50.767910   25572 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1218 16:55:50.768427   25572 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1218 16:55:50.795190   25572 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I1218 16:55:50.804193   25572 binaries.go:44] Found k8s binaries, skipping transfer
I1218 16:55:50.826530   25572 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1218 16:55:50.834994   25572 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I1218 16:55:50.851355   25572 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1218 16:55:50.863464   25572 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I1218 16:55:50.893662   25572 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1218 16:55:50.900204   25572 certs.go:56] Setting up C:\Users\Houda\.minikube\profiles\minikube for IP: 192.168.49.2
I1218 16:55:50.900204   25572 certs.go:190] acquiring lock for shared ca certs: {Name:mk134a0594de6fa1a469c77e92f8eb6fbcfe1892 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1218 16:55:50.900760   25572 certs.go:199] skipping minikubeCA CA generation: C:\Users\Houda\.minikube\ca.key
I1218 16:55:50.901421   25572 certs.go:199] skipping proxyClientCA CA generation: C:\Users\Houda\.minikube\proxy-client-ca.key
I1218 16:55:50.902515   25572 certs.go:315] skipping minikube-user signed cert generation: C:\Users\Houda\.minikube\profiles\minikube\client.key
I1218 16:55:50.903600   25572 certs.go:315] skipping minikube signed cert generation: C:\Users\Houda\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I1218 16:55:50.904264   25572 certs.go:315] skipping aggregator signed cert generation: C:\Users\Houda\.minikube\profiles\minikube\proxy-client.key
I1218 16:55:50.905308   25572 certs.go:437] found cert: C:\Users\Houda\.minikube\certs\C:\Users\Houda\.minikube\certs\ca-key.pem (1675 bytes)
I1218 16:55:50.905308   25572 certs.go:437] found cert: C:\Users\Houda\.minikube\certs\C:\Users\Houda\.minikube\certs\ca.pem (1074 bytes)
I1218 16:55:50.905845   25572 certs.go:437] found cert: C:\Users\Houda\.minikube\certs\C:\Users\Houda\.minikube\certs\cert.pem (1119 bytes)
I1218 16:55:50.905845   25572 certs.go:437] found cert: C:\Users\Houda\.minikube\certs\C:\Users\Houda\.minikube\certs\key.pem (1675 bytes)
I1218 16:55:50.913134   25572 ssh_runner.go:362] scp C:\Users\Houda\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1218 16:55:50.930273   25572 ssh_runner.go:362] scp C:\Users\Houda\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1218 16:55:50.947577   25572 ssh_runner.go:362] scp C:\Users\Houda\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1218 16:55:50.966421   25572 ssh_runner.go:362] scp C:\Users\Houda\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1218 16:55:50.982320   25572 ssh_runner.go:362] scp C:\Users\Houda\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1218 16:55:51.000629   25572 ssh_runner.go:362] scp C:\Users\Houda\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1218 16:55:51.016837   25572 ssh_runner.go:362] scp C:\Users\Houda\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1218 16:55:51.033678   25572 ssh_runner.go:362] scp C:\Users\Houda\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1218 16:55:51.049891   25572 ssh_runner.go:362] scp C:\Users\Houda\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1218 16:55:51.064896   25572 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1218 16:55:51.094493   25572 ssh_runner.go:195] Run: openssl version
I1218 16:55:51.126676   25572 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1218 16:55:51.160312   25572 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1218 16:55:51.165221   25572 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Nov 30 18:42 /usr/share/ca-certificates/minikubeCA.pem
I1218 16:55:51.183271   25572 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1218 16:55:51.204098   25572 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1218 16:55:51.232231   25572 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I1218 16:55:51.246617   25572 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1218 16:55:51.269159   25572 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1218 16:55:51.289515   25572 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1218 16:55:51.316246   25572 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1218 16:55:51.335461   25572 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1218 16:55:51.353968   25572 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1218 16:55:51.361044   25572 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Houda:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1218 16:55:51.364426   25572 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1218 16:55:51.406567   25572 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1218 16:55:51.415142   25572 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I1218 16:55:51.415735   25572 kubeadm.go:636] restartCluster start
I1218 16:55:51.427140   25572 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1218 16:55:51.435658   25572 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1218 16:55:51.439034   25572 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1218 16:55:51.547590   25572 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:62145"
I1218 16:55:51.571533   25572 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1218 16:55:51.579999   25572 api_server.go:166] Checking apiserver status ...
I1218 16:55:51.590986   25572 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1218 16:55:51.600423   25572 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1218 16:55:51.600948   25572 api_server.go:166] Checking apiserver status ...
I1218 16:55:51.612653   25572 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1218 16:55:51.622274   25572 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1218 16:55:52.124096   25572 api_server.go:166] Checking apiserver status ...
I1218 16:55:52.141342   25572 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1218 16:55:52.302369   25572 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1218 16:55:52.623847   25572 api_server.go:166] Checking apiserver status ...
I1218 16:55:52.646487   25572 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1218 16:55:52.804700   25572 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1218 16:55:53.135695   25572 api_server.go:166] Checking apiserver status ...
I1218 16:55:53.160262   25572 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1218 16:55:53.303462   25572 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1218 16:55:53.630069   25572 api_server.go:166] Checking apiserver status ...
I1218 16:55:53.649503   25572 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1218 16:55:53.802100   25572 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1218 16:55:54.125727   25572 api_server.go:166] Checking apiserver status ...
I1218 16:55:54.138163   25572 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1218 16:55:54.208768   25572 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1218 16:55:54.623864   25572 api_server.go:166] Checking apiserver status ...
I1218 16:55:54.650321   25572 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1218 16:55:54.712055   25572 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1218 16:55:55.136524   25572 api_server.go:166] Checking apiserver status ...
I1218 16:55:55.157900   25572 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1218 16:55:55.421861   25572 ssh_runner.go:195] Run: sudo egrep ^[0-9]+:freezer: /proc/407866/cgroup
I1218 16:55:55.903852   25572 api_server.go:182] apiserver freezer: "21:freezer:/docker/d63a5d5e9032a85140bc775399d1095e5a2fe1a0a6f94db22f59c3e6cd45351c/kubepods/burstable/pod55b4bbe24dac3803a7379f9ae169d6ba/a3da884807b5d942b784e48943c053504ed15ace501573a1256c5bfa0f5c4548"
I1218 16:55:55.927195   25572 ssh_runner.go:195] Run: sudo cat /sys/fs/cgroup/freezer/docker/d63a5d5e9032a85140bc775399d1095e5a2fe1a0a6f94db22f59c3e6cd45351c/kubepods/burstable/pod55b4bbe24dac3803a7379f9ae169d6ba/a3da884807b5d942b784e48943c053504ed15ace501573a1256c5bfa0f5c4548/freezer.state
I1218 16:55:56.602588   25572 api_server.go:204] freezer state: "THAWED"
I1218 16:55:56.602588   25572 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62145/healthz ...
I1218 16:55:56.606409   25572 api_server.go:269] stopped: https://127.0.0.1:62145/healthz: Get "https://127.0.0.1:62145/healthz": EOF
I1218 16:55:56.606916   25572 retry.go:31] will retry after 272.306593ms: state is "Stopped"
I1218 16:55:56.881044   25572 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62145/healthz ...
I1218 16:55:56.886872   25572 api_server.go:269] stopped: https://127.0.0.1:62145/healthz: Get "https://127.0.0.1:62145/healthz": EOF
I1218 16:55:56.886872   25572 retry.go:31] will retry after 249.429212ms: state is "Stopped"
I1218 16:55:57.143765   25572 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62145/healthz ...
I1218 16:56:02.156488   25572 api_server.go:269] stopped: https://127.0.0.1:62145/healthz: Get "https://127.0.0.1:62145/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1218 16:56:02.156488   25572 retry.go:31] will retry after 315.481769ms: state is "Stopped"
I1218 16:56:02.484777   25572 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62145/healthz ...
I1218 16:56:07.499145   25572 api_server.go:269] stopped: https://127.0.0.1:62145/healthz: Get "https://127.0.0.1:62145/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1218 16:56:07.499145   25572 retry.go:31] will retry after 442.205234ms: state is "Stopped"
I1218 16:56:07.949652   25572 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62145/healthz ...
I1218 16:56:08.013138   25572 api_server.go:279] https://127.0.0.1:62145/healthz returned 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1218 16:56:08.013138   25572 retry.go:31] will retry after 466.703973ms: https://127.0.0.1:62145/healthz returned error 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1218 16:56:08.494920   25572 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62145/healthz ...
I1218 16:56:08.509397   25572 api_server.go:279] https://127.0.0.1:62145/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1218 16:56:08.509397   25572 retry.go:31] will retry after 613.479197ms: https://127.0.0.1:62145/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1218 16:56:09.130103   25572 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62145/healthz ...
I1218 16:56:09.138383   25572 api_server.go:279] https://127.0.0.1:62145/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1218 16:56:09.138383   25572 retry.go:31] will retry after 1.103800493s: https://127.0.0.1:62145/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1218 16:56:10.253948   25572 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62145/healthz ...
I1218 16:56:10.261575   25572 api_server.go:279] https://127.0.0.1:62145/healthz returned 200:
ok
I1218 16:56:10.291296   25572 system_pods.go:86] 7 kube-system pods found
I1218 16:56:10.291296   25572 system_pods.go:89] "coredns-5dd5756b68-8hjhv" [b2e186e7-3b9b-4a91-a157-3f855d648793] Running
I1218 16:56:10.291296   25572 system_pods.go:89] "etcd-minikube" [cbe0849a-8cc0-4351-86cf-529d881fc4ca] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1218 16:56:10.291296   25572 system_pods.go:89] "kube-apiserver-minikube" [c0b119fc-ff76-4830-b479-7338c2aa58dd] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1218 16:56:10.291296   25572 system_pods.go:89] "kube-controller-manager-minikube" [ed1b016a-02ef-41e4-a5d2-46c3ce9f5f5b] Running
I1218 16:56:10.291296   25572 system_pods.go:89] "kube-proxy-967ql" [c705b2d4-948c-4553-9ba8-ba59e081f97b] Running
I1218 16:56:10.291296   25572 system_pods.go:89] "kube-scheduler-minikube" [a4e8970a-6d26-4b30-a8a8-dd9988258ce5] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1218 16:56:10.291296   25572 system_pods.go:89] "storage-provisioner" [bf3c8204-987c-46e2-b58e-5a9aa1d96782] Running
I1218 16:56:10.293024   25572 api_server.go:141] control plane version: v1.28.3
I1218 16:56:10.293024   25572 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I1218 16:56:10.293024   25572 kubeadm.go:684] Taking a shortcut, as the cluster seems to be properly configured
I1218 16:56:10.294134   25572 kubeadm.go:640] restartCluster took 18.8777916s
I1218 16:56:10.294134   25572 kubeadm.go:406] StartCluster complete in 18.9330908s
I1218 16:56:10.294134   25572 settings.go:142] acquiring lock: {Name:mk690c9142b669fb8f66a2af22536fd61bfdb89b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1218 16:56:10.294134   25572 settings.go:150] Updating kubeconfig:  C:\Users\Houda\.kube\config
I1218 16:56:10.295161   25572 lock.go:35] WriteFile acquiring C:\Users\Houda\.kube\config: {Name:mk74ab4cb894166e9614ffc19ba3fc0c7a79f9d0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1218 16:56:10.296798   25572 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1218 16:56:10.296798   25572 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1218 16:56:10.297329   25572 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I1218 16:56:10.297329   25572 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1218 16:56:10.297329   25572 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1218 16:56:10.297329   25572 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W1218 16:56:10.297329   25572 addons.go:240] addon storage-provisioner should already be in state true
I1218 16:56:10.297329   25572 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1218 16:56:10.297329   25572 host.go:66] Checking if "minikube" exists ...
I1218 16:56:10.317882   25572 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1218 16:56:10.317882   25572 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I1218 16:56:10.320513   25572 out.go:177] * Verifying Kubernetes components...
I1218 16:56:10.321037   25572 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1218 16:56:10.323691   25572 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1218 16:56:10.346302   25572 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1218 16:56:10.423024   25572 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1218 16:56:10.423562   25572 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1218 16:56:10.424081   25572 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1218 16:56:10.428556   25572 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1218 16:56:10.437984   25572 addons.go:231] Setting addon default-storageclass=true in "minikube"
W1218 16:56:10.437984   25572 addons.go:240] addon default-storageclass should already be in state true
I1218 16:56:10.437984   25572 host.go:66] Checking if "minikube" exists ...
I1218 16:56:10.455089   25572 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1218 16:56:10.461406   25572 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1218 16:56:10.465245   25572 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1218 16:56:10.544029   25572 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62146 SSHKeyPath:C:\Users\Houda\.minikube\machines\minikube\id_rsa Username:docker}
I1218 16:56:10.556415   25572 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I1218 16:56:10.556415   25572 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1218 16:56:10.562408   25572 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1218 16:56:10.574678   25572 api_server.go:52] waiting for apiserver process to appear ...
I1218 16:56:10.600667   25572 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1218 16:56:10.614122   25572 api_server.go:72] duration metric: took 296.2409ms to wait for apiserver process to appear ...
I1218 16:56:10.614122   25572 api_server.go:88] waiting for apiserver healthz status ...
I1218 16:56:10.614122   25572 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:62145/healthz ...
I1218 16:56:10.620112   25572 api_server.go:279] https://127.0.0.1:62145/healthz returned 200:
ok
I1218 16:56:10.622217   25572 api_server.go:141] control plane version: v1.28.3
I1218 16:56:10.622217   25572 api_server.go:131] duration metric: took 8.0947ms to wait for apiserver health ...
I1218 16:56:10.622739   25572 system_pods.go:43] waiting for kube-system pods to appear ...
I1218 16:56:10.627536   25572 system_pods.go:59] 7 kube-system pods found
I1218 16:56:10.627536   25572 system_pods.go:61] "coredns-5dd5756b68-8hjhv" [b2e186e7-3b9b-4a91-a157-3f855d648793] Running
I1218 16:56:10.627536   25572 system_pods.go:61] "etcd-minikube" [cbe0849a-8cc0-4351-86cf-529d881fc4ca] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1218 16:56:10.627536   25572 system_pods.go:61] "kube-apiserver-minikube" [c0b119fc-ff76-4830-b479-7338c2aa58dd] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1218 16:56:10.627536   25572 system_pods.go:61] "kube-controller-manager-minikube" [ed1b016a-02ef-41e4-a5d2-46c3ce9f5f5b] Running
I1218 16:56:10.627536   25572 system_pods.go:61] "kube-proxy-967ql" [c705b2d4-948c-4553-9ba8-ba59e081f97b] Running
I1218 16:56:10.627536   25572 system_pods.go:61] "kube-scheduler-minikube" [a4e8970a-6d26-4b30-a8a8-dd9988258ce5] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1218 16:56:10.627536   25572 system_pods.go:61] "storage-provisioner" [bf3c8204-987c-46e2-b58e-5a9aa1d96782] Running
I1218 16:56:10.627536   25572 system_pods.go:74] duration metric: took 4.7973ms to wait for pod list to return data ...
I1218 16:56:10.627536   25572 kubeadm.go:581] duration metric: took 309.6543ms to wait for : map[apiserver:true system_pods:true] ...
I1218 16:56:10.628083   25572 node_conditions.go:102] verifying NodePressure condition ...
I1218 16:56:10.632922   25572 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1218 16:56:10.632922   25572 node_conditions.go:123] node cpu capacity is 20
I1218 16:56:10.633518   25572 node_conditions.go:105] duration metric: took 5.4352ms to run NodePressure ...
I1218 16:56:10.633518   25572 start.go:228] waiting for startup goroutines ...
I1218 16:56:10.654653   25572 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1218 16:56:10.666152   25572 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:62146 SSHKeyPath:C:\Users\Houda\.minikube\machines\minikube\id_rsa Username:docker}
I1218 16:56:10.771731   25572 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1218 16:56:12.045280   25572 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.3906268s)
I1218 16:56:12.045280   25572 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.2735486s)
I1218 16:56:12.116776   25572 out.go:177] * Enabled addons: storage-provisioner, default-storageclass
I1218 16:56:12.117921   25572 addons.go:502] enable addons completed in 1.8211226s: enabled=[storage-provisioner default-storageclass]
I1218 16:56:12.117921   25572 start.go:233] waiting for cluster config update ...
I1218 16:56:12.117921   25572 start.go:242] writing updated cluster config ...
I1218 16:56:12.131324   25572 ssh_runner.go:195] Run: rm -f paused
I1218 16:56:12.226716   25572 start.go:600] kubectl: 1.25.9, cluster: 1.28.3 (minor skew: 3)
I1218 16:56:12.227828   25572 out.go:177] 
W1218 16:56:12.230190   25572 out.go:239] ! C:\Program Files\Docker\Docker\resources\bin\kubectl.exe is version 1.25.9, which may have incompatibilities with Kubernetes 1.28.3.
I1218 16:56:12.231781   25572 out.go:177]   - Want kubectl v1.28.3? Try 'minikube kubectl -- get pods -A'
I1218 16:56:12.233619   25572 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Dec 18 19:09:04 minikube dockerd[405731]: time="2023-12-18T19:09:04.929595249Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:09:04 minikube dockerd[405731]: time="2023-12-18T19:09:04.929647088Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:09:10 minikube dockerd[405731]: time="2023-12-18T19:09:10.584413498Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:09:10 minikube dockerd[405731]: time="2023-12-18T19:09:10.584499929Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:09:13 minikube dockerd[405731]: time="2023-12-18T19:09:13.369248850Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:09:13 minikube dockerd[405731]: time="2023-12-18T19:09:13.369292995Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:09:15 minikube dockerd[405731]: time="2023-12-18T19:09:15.946163153Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:09:15 minikube dockerd[405731]: time="2023-12-18T19:09:15.946194494Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:09:20 minikube dockerd[405731]: time="2023-12-18T19:09:20.260360994Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:09:20 minikube dockerd[405731]: time="2023-12-18T19:09:20.260416571Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:09:33 minikube dockerd[405731]: time="2023-12-18T19:09:33.849217244Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:09:33 minikube dockerd[405731]: time="2023-12-18T19:09:33.849264391Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:09:43 minikube dockerd[405731]: time="2023-12-18T19:09:43.793242032Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:09:43 minikube dockerd[405731]: time="2023-12-18T19:09:43.793412319Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:09:47 minikube dockerd[405731]: time="2023-12-18T19:09:47.472969468Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:09:47 minikube dockerd[405731]: time="2023-12-18T19:09:47.473073803Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:09:51 minikube dockerd[405731]: time="2023-12-18T19:09:51.570360175Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:09:51 minikube dockerd[405731]: time="2023-12-18T19:09:51.570484114Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:09:54 minikube dockerd[405731]: time="2023-12-18T19:09:54.971409002Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:09:54 minikube dockerd[405731]: time="2023-12-18T19:09:54.971865853Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:10:19 minikube dockerd[405731]: time="2023-12-18T19:10:19.191832024Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:10:19 minikube dockerd[405731]: time="2023-12-18T19:10:19.191982717Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:10:30 minikube dockerd[405731]: time="2023-12-18T19:10:30.057106331Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:10:30 minikube dockerd[405731]: time="2023-12-18T19:10:30.057215409Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:10:37 minikube dockerd[405731]: time="2023-12-18T19:10:37.646891524Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:10:37 minikube dockerd[405731]: time="2023-12-18T19:10:37.646993051Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:10:41 minikube dockerd[405731]: time="2023-12-18T19:10:41.332264293Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:10:41 minikube dockerd[405731]: time="2023-12-18T19:10:41.332380052Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:10:53 minikube dockerd[405731]: time="2023-12-18T19:10:53.911834964Z" level=info msg="Attempting next endpoint for pull after error: Head \"https://registry-1.docker.io/v2/houda24/projectdevopsmicro/manifests/latest\": net/http: TLS handshake timeout"
Dec 18 19:10:53 minikube dockerd[405731]: time="2023-12-18T19:10:53.918722700Z" level=error msg="Handler for POST /v1.42/images/create returned error: Head \"https://registry-1.docker.io/v2/houda24/projectdevopsmicro/manifests/latest\": net/http: TLS handshake timeout"
Dec 18 19:11:48 minikube dockerd[405731]: time="2023-12-18T19:11:48.912228242Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:11:48 minikube dockerd[405731]: time="2023-12-18T19:11:48.912318125Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:12:07 minikube dockerd[405731]: time="2023-12-18T19:12:07.685844371Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:12:07 minikube dockerd[405731]: time="2023-12-18T19:12:07.685905065Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:12:11 minikube dockerd[405731]: time="2023-12-18T19:12:11.828680459Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:12:11 minikube dockerd[405731]: time="2023-12-18T19:12:11.828766736Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:12:15 minikube dockerd[405731]: time="2023-12-18T19:12:15.779161238Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:12:15 minikube dockerd[405731]: time="2023-12-18T19:12:15.779247265Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:12:26 minikube dockerd[405731]: time="2023-12-18T19:12:26.389593892Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:12:26 minikube dockerd[405731]: time="2023-12-18T19:12:26.389634574Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:13:12 minikube dockerd[405731]: time="2023-12-18T19:13:12.844188181Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:13:12 minikube dockerd[405731]: time="2023-12-18T19:13:12.844294747Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:13:32 minikube dockerd[405731]: time="2023-12-18T19:13:32.983823281Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:13:32 minikube dockerd[405731]: time="2023-12-18T19:13:32.983874721Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:14:28 minikube dockerd[405731]: time="2023-12-18T19:14:28.486620055Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:14:28 minikube dockerd[405731]: time="2023-12-18T19:14:28.486689386Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:14:41 minikube dockerd[405731]: time="2023-12-18T19:14:41.911282063Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:14:41 minikube dockerd[405731]: time="2023-12-18T19:14:41.911330157Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:15:04 minikube dockerd[405731]: time="2023-12-18T19:15:04.441214600Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:15:04 minikube dockerd[405731]: time="2023-12-18T19:15:04.441260822Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:15:07 minikube dockerd[405731]: time="2023-12-18T19:15:07.839224938Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:15:07 minikube dockerd[405731]: time="2023-12-18T19:15:07.839265967Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:15:10 minikube dockerd[405731]: time="2023-12-18T19:15:10.919320004Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:15:10 minikube dockerd[405731]: time="2023-12-18T19:15:10.920295117Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:15:20 minikube dockerd[405731]: time="2023-12-18T19:15:20.536320605Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:15:20 minikube dockerd[405731]: time="2023-12-18T19:15:20.536466576Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:18:21 minikube dockerd[405731]: time="2023-12-18T19:18:21.444882416Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:18:21 minikube dockerd[405731]: time="2023-12-18T19:18:21.444994292Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Dec 18 19:18:45 minikube dockerd[405731]: time="2023-12-18T19:18:45.959342691Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Dec 18 19:18:45 minikube dockerd[405731]: time="2023-12-18T19:18:45.959389120Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                             CREATED             STATE               NAME                                 ATTEMPT             POD ID              POD
03185e9cf2bdc       nginx@sha256:10d1f5b58f74683ad34eb29287e07dab1e90f10af243f151bb50aa5dbb4d62ee                     3 hours ago         Running             nginx                                0                   6be11a5287943       nginx-demo-new-6b4d9b5484-dxd9r
75f43603bea9b       nginx@sha256:10d1f5b58f74683ad34eb29287e07dab1e90f10af243f151bb50aa5dbb4d62ee                     3 hours ago         Running             nginx                                0                   b96be565ac3d6       nginx-demo-new-6b4d9b5484-42x5c
8afc4f4f4084e       nginx@sha256:10d1f5b58f74683ad34eb29287e07dab1e90f10af243f151bb50aa5dbb4d62ee                     3 hours ago         Running             nginx                                0                   92726722a1b41       nginx-demo-new-6b4d9b5484-vs8xl
bee5cf434ff46       nginx@sha256:10d1f5b58f74683ad34eb29287e07dab1e90f10af243f151bb50aa5dbb4d62ee                     3 hours ago         Running             nginx                                0                   77f90c838f205       nginx-demo-new-6b4d9b5484-slwjp
3dd6869bcf0d1       nginx@sha256:10d1f5b58f74683ad34eb29287e07dab1e90f10af243f151bb50aa5dbb4d62ee                     3 hours ago         Running             nginx                                0                   639758a1a4c87       nginx-demo-new-6b4d9b5484-k4js6
992b602d3f539       ghcr.io/dexidp/dex@sha256:f579d00721b0d842328c43a562f50343c54b0048ef2d58d6b54e750c21fc7938        3 hours ago         Running             dex                                  5                   b2a9439046a23       argocd-dex-server-7bb445db59-5jxhs
68751def19c5f       quay.io/argoproj/argocd@sha256:dfc13f4b9ddbf25a88f7018667ee45e35a49520e2716242e3f65ef75f88c25ff   3 hours ago         Running             argocd-repo-server                   5                   3baa6e8a278cc       argocd-repo-server-6d8d59bbc7-nxkgh
d5b914bed6d26       quay.io/argoproj/argocd@sha256:dfc13f4b9ddbf25a88f7018667ee45e35a49520e2716242e3f65ef75f88c25ff   3 hours ago         Exited              copyutil                             1                   b2a9439046a23       argocd-dex-server-7bb445db59-5jxhs
42bb09fb6a459       nginx@sha256:10d1f5b58f74683ad34eb29287e07dab1e90f10af243f151bb50aa5dbb4d62ee                     3 hours ago         Running             nginx                                7                   bdcf62aad4b68       nginx-demo-554db85f85-s6ptp
71b90ed313e3f       nginx@sha256:10d1f5b58f74683ad34eb29287e07dab1e90f10af243f151bb50aa5dbb4d62ee                     3 hours ago         Running             nginx                                7                   68e9c716cfe25       nginx-demo-554db85f85-4sn9b
0a18f66060dfe       nginx@sha256:10d1f5b58f74683ad34eb29287e07dab1e90f10af243f151bb50aa5dbb4d62ee                     3 hours ago         Running             nginx                                7                   3f34d3c15787a       nginx-demo-554db85f85-kjn5x
5a307dae6f968       quay.io/argoproj/argocd@sha256:dfc13f4b9ddbf25a88f7018667ee45e35a49520e2716242e3f65ef75f88c25ff   3 hours ago         Running             argocd-application-controller        5                   332c1c9a3fb87       argocd-application-controller-0
2d4242c4f910a       nginx@sha256:10d1f5b58f74683ad34eb29287e07dab1e90f10af243f151bb50aa5dbb4d62ee                     3 hours ago         Running             nginx                                7                   af3ecac96de0f       nginx-demo-554db85f85-mwrpv
8b35ad3fa0773       quay.io/argoproj/argocd@sha256:dfc13f4b9ddbf25a88f7018667ee45e35a49520e2716242e3f65ef75f88c25ff   3 hours ago         Running             argocd-applicationset-controller     5                   cb70528138c1a       argocd-applicationset-controller-5f975ff5-9lbwk
2320651d8c154       quay.io/argoproj/argocd@sha256:dfc13f4b9ddbf25a88f7018667ee45e35a49520e2716242e3f65ef75f88c25ff   3 hours ago         Running             argocd-server                        5                   652ef3913cc1f       argocd-server-58f5668765-d5xqv
ca2abb9c7e49a       quay.io/argoproj/argocd@sha256:dfc13f4b9ddbf25a88f7018667ee45e35a49520e2716242e3f65ef75f88c25ff   3 hours ago         Running             argocd-notifications-controller      5                   e52e4707d3188       argocd-notifications-controller-566465df76-lsmrz
45b701a5a4cb3       redis@sha256:121bac949fb5f623b9fa0b4e4c9fb358ffd045966e754cfa3eb9963f3af2fe3b                     3 hours ago         Running             redis                                6                   90e6554a383cd       argocd-redis-6976fc7dfc-slpr9
795df3f34eecb       620d5e2a39df1                                                                                     3 hours ago         Running             prometheus-server                    4                   06797d1616270       prometheus-server-8fffdb69d-wvt4r
7202bfbc86c45       nginx@sha256:10d1f5b58f74683ad34eb29287e07dab1e90f10af243f151bb50aa5dbb4d62ee                     3 hours ago         Running             nginx                                7                   bad477d4d9dae       nginx-demo-554db85f85-42dpt
6238254aea069       ead0a4a53df89                                                                                     3 hours ago         Running             coredns                              8                   9974fea6d64d3       coredns-5dd5756b68-8hjhv
cecfc485b9102       53b45390b4d28                                                                                     3 hours ago         Running             prometheus-server-configmap-reload   4                   06797d1616270       prometheus-server-8fffdb69d-wvt4r
64573a4de13e9       9f27df16978d8                                                                                     3 hours ago         Running             alertmanager                         4                   823c459dbc580       prometheus-alertmanager-0
6fa8415356294       568d57c0919bf                                                                                     3 hours ago         Exited              copyutil                             5                   3baa6e8a278cc       argocd-repo-server-6d8d59bbc7-nxkgh
90df7bbbd53a9       a30191ce4597f                                                                                     3 hours ago         Running             pushgateway                          4                   7e3cb2814ad7f       prometheus-prometheus-pushgateway-7857c44f49-fwzws
f1c647a8d5f35       f5f9f4739d5f4                                                                                     3 hours ago         Running             kube-state-metrics                   6                   734e308de3fc3       prometheus-kube-state-metrics-6b464f5b88-8c7r8
625d30a1064a4       1f75a69704e1f                                                                                     3 hours ago         Running             nginx                                5                   6f6e2eeee7146       my-nginx-64666fbb86-cp7xk
881a69ac79215       5c51d2b27fc48                                                                                     3 hours ago         Running             kube-state-metrics                   6                   8c89d895677d7       kube-state-metrics-7589748fd9-sbcpt
dedb6961b8d09       6d1b4fd1b182d                                                                                     3 hours ago         Running             kube-scheduler                       8                   bbe839d993f7a       kube-scheduler-minikube
135bf9fcff6f8       6e38f40d628db                                                                                     3 hours ago         Running             storage-provisioner                  14                  3262a5631ee0b       storage-provisioner
2bf4e64c554de       72c9c20889862                                                                                     3 hours ago         Running             node-exporter                        4                   38ba351a141a7       prometheus-prometheus-node-exporter-n89mh
643eb0a8cf4c4       73deb9a3f7025                                                                                     3 hours ago         Running             etcd                                 8                   da149f5b328e3       etcd-minikube
a3da884807b5d       5374347291230                                                                                     3 hours ago         Running             kube-apiserver                       9                   07c7e27afb6c7       kube-apiserver-minikube
6b6ef07eb9003       bfc896cf80fba                                                                                     3 hours ago         Running             kube-proxy                           8                   9c63e9d63c10e       kube-proxy-967ql
b4022bf23bb61       10baa1ca17068                                                                                     3 hours ago         Running             kube-controller-manager              9                   dcca8e7d05a84       kube-controller-manager-minikube
37537a64ec4f0       ghcr.io/dexidp/dex@sha256:f579d00721b0d842328c43a562f50343c54b0048ef2d58d6b54e750c21fc7938        5 hours ago         Exited              dex                                  4                   a5eb942c71847       argocd-dex-server-7bb445db59-5jxhs
d2a9212ec3d3a       quay.io/argoproj/argocd@sha256:dfc13f4b9ddbf25a88f7018667ee45e35a49520e2716242e3f65ef75f88c25ff   5 hours ago         Exited              argocd-application-controller        4                   53221f4fbbf91       argocd-application-controller-0
14fbe06774cda       quay.io/argoproj/argocd@sha256:dfc13f4b9ddbf25a88f7018667ee45e35a49520e2716242e3f65ef75f88c25ff   5 hours ago         Exited              argocd-server                        4                   a624bb811a241       argocd-server-58f5668765-d5xqv
2924405dfef29       quay.io/argoproj/argocd@sha256:dfc13f4b9ddbf25a88f7018667ee45e35a49520e2716242e3f65ef75f88c25ff   5 hours ago         Exited              argocd-repo-server                   4                   c7d475904e9db       argocd-repo-server-6d8d59bbc7-nxkgh
9726f6e544569       quay.io/argoproj/argocd@sha256:dfc13f4b9ddbf25a88f7018667ee45e35a49520e2716242e3f65ef75f88c25ff   5 hours ago         Exited              argocd-notifications-controller      4                   46856eea588fd       argocd-notifications-controller-566465df76-lsmrz
a874ffe69284c       redis@sha256:121bac949fb5f623b9fa0b4e4c9fb358ffd045966e754cfa3eb9963f3af2fe3b                     5 hours ago         Exited              redis                                5                   bd9a8e424f921       argocd-redis-6976fc7dfc-slpr9
68952c4e57b98       nginx@sha256:10d1f5b58f74683ad34eb29287e07dab1e90f10af243f151bb50aa5dbb4d62ee                     5 hours ago         Exited              nginx                                6                   b01ea884959a3       nginx-demo-554db85f85-42dpt
044b640ef98a7       nginx@sha256:10d1f5b58f74683ad34eb29287e07dab1e90f10af243f151bb50aa5dbb4d62ee                     5 hours ago         Exited              nginx                                6                   a93c0d16bed05       nginx-demo-554db85f85-mwrpv
ccb66d4c99812       6e38f40d628db                                                                                     5 hours ago         Exited              storage-provisioner                  13                  cf7031bba2934       storage-provisioner
65fb5ee97e358       5c51d2b27fc48                                                                                     5 hours ago         Exited              kube-state-metrics                   5                   cb7e0170654f7       kube-state-metrics-7589748fd9-sbcpt
3fa80df3f12d0       f5f9f4739d5f4                                                                                     5 hours ago         Exited              kube-state-metrics                   5                   99e1fc3e7f700       prometheus-kube-state-metrics-6b464f5b88-8c7r8
f49676c68eba4       bfc896cf80fba                                                                                     5 hours ago         Exited              kube-proxy                           7                   2f0c6a85cf05f       kube-proxy-967ql
b5104a3804ed6       ead0a4a53df89                                                                                     5 hours ago         Exited              coredns                              7                   9984ca3ce4de1       coredns-5dd5756b68-8hjhv
7b3e31c874c4b       5374347291230                                                                                     5 hours ago         Exited              kube-apiserver                       8                   198d254f5bc7f       kube-apiserver-minikube
0a9e08cdbaadb       73deb9a3f7025                                                                                     5 hours ago         Exited              etcd                                 7                   150317828e86d       etcd-minikube
a6a599c0f82c3       6d1b4fd1b182d                                                                                     5 hours ago         Exited              kube-scheduler                       7                   a501d3ddd5431       kube-scheduler-minikube
d60853a3eb048       10baa1ca17068                                                                                     5 hours ago         Exited              kube-controller-manager              8                   b47492ab377a5       kube-controller-manager-minikube
a6b404e53bc85       nginx@sha256:10d1f5b58f74683ad34eb29287e07dab1e90f10af243f151bb50aa5dbb4d62ee                     5 hours ago         Exited              nginx                                6                   100cd0d2f0eb1       nginx-demo-554db85f85-s6ptp
c0e43d1f6901c       quay.io/argoproj/argocd@sha256:dfc13f4b9ddbf25a88f7018667ee45e35a49520e2716242e3f65ef75f88c25ff   5 hours ago         Exited              argocd-applicationset-controller     4                   241fec8e6b23a       argocd-applicationset-controller-5f975ff5-9lbwk
bcbd8fbf7d0ba       nginx@sha256:10d1f5b58f74683ad34eb29287e07dab1e90f10af243f151bb50aa5dbb4d62ee                     5 hours ago         Exited              nginx                                6                   9b5a09a6cc79e       nginx-demo-554db85f85-4sn9b
e7fc294260270       620d5e2a39df1                                                                                     5 hours ago         Exited              prometheus-server                    3                   f76b1aef916d6       prometheus-server-8fffdb69d-wvt4r
d37897aab3ce0       nginx@sha256:10d1f5b58f74683ad34eb29287e07dab1e90f10af243f151bb50aa5dbb4d62ee                     5 hours ago         Exited              nginx                                6                   2636bd996aee6       nginx-demo-554db85f85-kjn5x
9352ccb21614f       9f27df16978d8                                                                                     5 hours ago         Exited              alertmanager                         3                   d7cc889b8984c       prometheus-alertmanager-0
47a60672dad20       53b45390b4d28                                                                                     5 hours ago         Exited              prometheus-server-configmap-reload   3                   f76b1aef916d6       prometheus-server-8fffdb69d-wvt4r
c952fabf2b591       a30191ce4597f                                                                                     5 hours ago         Exited              pushgateway                          3                   23525c1f1745b       prometheus-prometheus-pushgateway-7857c44f49-fwzws
04bd35036d70b       1f75a69704e1f                                                                                     5 hours ago         Exited              nginx                                4                   dfce3fab69735       my-nginx-64666fbb86-cp7xk
edcbee68ab918       72c9c20889862                                                                                     5 hours ago         Exited              node-exporter                        3                   f5129db520826       prometheus-prometheus-node-exporter-n89mh

* 
* ==> coredns [6238254aea06] <==
* [INFO] 10.244.0.136:50232 - 59195 "AAAA IN argocd-redis.argocd.svc.cluster.local. udp 66 false 1232" NOERROR qr,aa,rd 148 0.000574919s
[INFO] 10.244.0.136:49354 - 3967 "A IN argocd-redis.argocd.svc.cluster.local. udp 66 false 1232" NOERROR qr,aa,rd 108 0.000781117s
[INFO] 10.244.0.146:54138 - 37346 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000293085s
[INFO] 10.244.0.146:51044 - 34896 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000415923s
[INFO] 10.244.0.138:54827 - 30086 "A IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000208734s
[INFO] 10.244.0.138:48841 - 62166 "AAAA IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000276156s
[INFO] 10.244.0.138:46781 - 64979 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000091919s
[INFO] 10.244.0.138:52572 - 7358 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000158661s
[INFO] 10.244.0.138:45663 - 54187 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000242051s
[INFO] 10.244.0.138:45520 - 45320 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000312146s
[INFO] 10.244.0.138:54235 - 52113 "A IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.002813915s
[INFO] 10.244.0.138:44055 - 58774 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 112 0.300755969s
[INFO] 10.244.0.138:52871 - 12055 "AAAA IN github.com.argocd.svc.cluster.local. udp 53 false 512" NXDOMAIN qr,aa,rd 146 0.000189271s
[INFO] 10.244.0.138:52871 - 46354 "A IN github.com.argocd.svc.cluster.local. udp 53 false 512" NXDOMAIN qr,aa,rd 146 0.000183749s
[INFO] 10.244.0.138:35563 - 45478 "AAAA IN github.com.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000074515s
[INFO] 10.244.0.138:35563 - 48800 "A IN github.com.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000101114s
[INFO] 10.244.0.138:54733 - 52675 "AAAA IN github.com.cluster.local. udp 42 false 512" NXDOMAIN qr,aa,rd 135 0.00004916s
[INFO] 10.244.0.138:54733 - 60609 "A IN github.com.cluster.local. udp 42 false 512" NXDOMAIN qr,aa,rd 135 0.000081141s
[INFO] 10.244.0.138:41659 - 50044 "AAAA IN github.com. udp 28 false 512" NOERROR qr,aa,rd,ra 112 0.000139764s
[INFO] 10.244.0.138:41659 - 54141 "A IN github.com. udp 28 false 512" NOERROR qr,aa,rd,ra 54 0.00019415s
[INFO] 10.244.0.146:45246 - 740 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000367909s
[INFO] 10.244.0.146:54077 - 18669 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000372765s
[INFO] 10.244.0.146:34610 - 23184 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.00014641s
[INFO] 10.244.0.146:55442 - 24458 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000168409s
[INFO] 10.244.0.146:39113 - 1180 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000609897s
[INFO] 10.244.0.146:56079 - 47174 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000817884s
[INFO] 10.244.0.138:47413 - 20679 "A IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000177059s
[INFO] 10.244.0.138:58381 - 6772 "AAAA IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000221927s
[INFO] 10.244.0.138:44717 - 46805 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000117375s
[INFO] 10.244.0.138:45448 - 56675 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.00018569s
[INFO] 10.244.0.138:39253 - 22502 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000112361s
[INFO] 10.244.0.138:51849 - 26521 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000223491s
[INFO] 10.244.0.138:46804 - 35628 "A IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.057505102s
[INFO] 10.244.0.138:49887 - 9093 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 112 0.057612062s
[INFO] 10.244.0.146:37688 - 46819 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000257752s
[INFO] 10.244.0.146:47260 - 12474 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000322116s
[INFO] 10.244.0.146:46831 - 58468 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000291293s
[INFO] 10.244.0.146:53961 - 16414 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000366785s
[INFO] 10.244.0.138:42101 - 45147 "A IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000128215s
[INFO] 10.244.0.138:51956 - 46143 "AAAA IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000142443s
[INFO] 10.244.0.138:39721 - 724 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000103207s
[INFO] 10.244.0.138:47372 - 53985 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000106984s
[INFO] 10.244.0.138:57876 - 23341 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000134337s
[INFO] 10.244.0.138:60596 - 40157 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000171105s
[INFO] 10.244.0.138:49805 - 34880 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 112 0.008632377s
[INFO] 10.244.0.138:58064 - 16231 "A IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.161052611s
[INFO] 10.244.0.146:51730 - 41789 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000347441s
[INFO] 10.244.0.146:38711 - 48605 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000513385s
[INFO] 10.244.0.146:45275 - 42762 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000415078s
[INFO] 10.244.0.146:37045 - 2098 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000453836s
[INFO] 10.244.0.138:38801 - 48049 "AAAA IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000108318s
[INFO] 10.244.0.138:59940 - 26471 "A IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000116235s
[INFO] 10.244.0.138:37307 - 56024 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000116518s
[INFO] 10.244.0.138:51702 - 24080 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000066097s
[INFO] 10.244.0.138:60849 - 8997 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000076357s
[INFO] 10.244.0.138:43378 - 35000 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000288232s
[INFO] 10.244.0.138:57286 - 18365 "A IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.065865628s
[INFO] 10.244.0.138:53073 - 10784 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 112 0.123270695s
[INFO] 10.244.0.146:46226 - 60501 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000438394s
[INFO] 10.244.0.146:57800 - 21681 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000251867s

* 
* ==> coredns [b5104a3804ed] <==
* [INFO] 10.244.0.123:48396 - 42627 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.00008508s
[INFO] 10.244.0.123:35425 - 65148 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000123724s
[INFO] 10.244.0.123:58870 - 45024 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000082697s
[INFO] 10.244.0.123:39115 - 12693 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000082113s
[INFO] 10.244.0.123:33564 - 56138 "A IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.006968178s
[INFO] 10.244.0.123:56531 - 47292 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 112 0.163954341s
[INFO] 10.244.0.123:39866 - 13617 "A IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000274794s
[INFO] 10.244.0.123:40823 - 46252 "AAAA IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000384214s
[INFO] 10.244.0.123:50927 - 22110 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000240737s
[INFO] 10.244.0.123:56776 - 22872 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000387254s
[INFO] 10.244.0.123:39578 - 31973 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000138307s
[INFO] 10.244.0.123:59482 - 63247 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000336325s
[INFO] 10.244.0.123:58155 - 20109 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,aa,rd,ra 112 0.000151438s
[INFO] 10.244.0.123:59258 - 44602 "A IN github.com. udp 39 false 1232" NOERROR qr,aa,rd,ra 54 0.000143525s
[INFO] 10.244.0.123:44182 - 45683 "AAAA IN github.com.argocd.svc.cluster.local. udp 53 false 512" NXDOMAIN qr,aa,rd 146 0.000099507s
[INFO] 10.244.0.123:44182 - 17777 "A IN github.com.argocd.svc.cluster.local. udp 53 false 512" NXDOMAIN qr,aa,rd 146 0.000139654s
[INFO] 10.244.0.123:39402 - 29143 "AAAA IN github.com.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000055398s
[INFO] 10.244.0.123:39402 - 52177 "A IN github.com.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000101675s
[INFO] 10.244.0.123:47938 - 37335 "AAAA IN github.com.cluster.local. udp 42 false 512" NXDOMAIN qr,aa,rd 135 0.000063692s
[INFO] 10.244.0.123:47938 - 7382 "A IN github.com.cluster.local. udp 42 false 512" NXDOMAIN qr,aa,rd 135 0.000111417s
[INFO] 10.244.0.123:55474 - 26385 "AAAA IN github.com. udp 28 false 512" NOERROR qr,aa,rd,ra 112 0.000039259s
[INFO] 10.244.0.123:55474 - 25104 "A IN github.com. udp 28 false 512" NOERROR qr,aa,rd,ra 54 0.000055588s
[INFO] 10.244.0.116:57814 - 29904 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.001072096s
[INFO] 10.244.0.116:44559 - 41978 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.001102339s
[INFO] 10.244.0.116:59876 - 43724 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000249081s
[INFO] 10.244.0.116:56144 - 50608 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000279169s
[INFO] 10.244.0.123:53816 - 1938 "AAAA IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000080508s
[INFO] 10.244.0.123:42708 - 48864 "A IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000081083s
[INFO] 10.244.0.123:50804 - 54916 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000093997s
[INFO] 10.244.0.123:38601 - 10176 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000127222s
[INFO] 10.244.0.123:37418 - 37195 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000098513s
[INFO] 10.244.0.123:55487 - 37483 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.00015132s
[INFO] 10.244.0.123:45762 - 46430 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 112 0.010442321s
[INFO] 10.244.0.123:36129 - 20303 "A IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.028878772s
[INFO] 10.244.0.116:36398 - 40226 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000671999s
[INFO] 10.244.0.116:55454 - 2551 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000500815s
[INFO] 10.244.0.116:35361 - 19967 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000161406s
[INFO] 10.244.0.116:52649 - 28608 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000257009s
[INFO] 10.244.0.123:49079 - 9769 "A IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.00010801s
[INFO] 10.244.0.123:46509 - 61603 "AAAA IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000142s
[INFO] 10.244.0.123:43625 - 45145 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000087689s
[INFO] 10.244.0.123:37526 - 42524 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.00010893s
[INFO] 10.244.0.123:50741 - 34561 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000078955s
[INFO] 10.244.0.123:60282 - 59723 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000101726s
[INFO] 10.244.0.123:45222 - 57700 "A IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.046453094s
[INFO] 10.244.0.123:45786 - 17410 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 112 0.051812225s
[INFO] 10.244.0.116:47521 - 64360 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000289738s
[INFO] 10.244.0.116:46492 - 28454 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000367446s
[INFO] 10.244.0.116:39565 - 50180 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000250289s
[INFO] 10.244.0.116:48308 - 13979 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000301054s
[INFO] 10.244.0.123:60788 - 57682 "AAAA IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000079224s
[INFO] 10.244.0.123:37502 - 3223 "A IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000090011s
[INFO] 10.244.0.123:51080 - 31628 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000080134s
[INFO] 10.244.0.123:60036 - 21200 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000234604s
[INFO] 10.244.0.123:51306 - 27046 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000098544s
[INFO] 10.244.0.123:47405 - 55846 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000075106s
[INFO] 10.244.0.123:41968 - 52985 "A IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.059902815s
[INFO] 10.244.0.123:55561 - 36879 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 112 0.073050856s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_11_30T19_42_15_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 30 Nov 2023 18:42:12 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 18 Dec 2023 19:18:53 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 18 Dec 2023 19:17:04 +0000   Thu, 30 Nov 2023 18:42:11 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 18 Dec 2023 19:17:04 +0000   Thu, 30 Nov 2023 18:42:11 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 18 Dec 2023 19:17:04 +0000   Thu, 30 Nov 2023 18:42:11 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 18 Dec 2023 19:17:04 +0000   Thu, 30 Nov 2023 18:42:12 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                20
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7973236Ki
  pods:               110
Allocatable:
  cpu:                20
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7973236Ki
  pods:               110
System Info:
  Machine ID:                 48dcdaa17db544a49bd443b8ed2761e9
  System UUID:                48dcdaa17db544a49bd443b8ed2761e9
  Boot ID:                    87e23b48-199d-4ed1-951a-91a770db65fd
  Kernel Version:             5.15.133.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (39 in total)
  Namespace                   Name                                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                                  ------------  ----------  ---------------  -------------  ---
  argocd                      argocd-application-controller-0                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
  argocd                      argocd-applicationset-controller-5f975ff5-9lbwk       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
  argocd                      argocd-dex-server-7bb445db59-5jxhs                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
  argocd                      argocd-notifications-controller-566465df76-lsmrz      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
  argocd                      argocd-redis-6976fc7dfc-slpr9                         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         17d
  argocd                      argocd-repo-server-6d8d59bbc7-nxkgh                   0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
  argocd                      argocd-server-58f5668765-d5xqv                        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
  default                     devopsprojectmicro-deployment-866c9d94b9-bhqv2        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  default                     devopsprojectmicro-deployment-866c9d94b9-fv75s        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  default                     devopsprojectmicro-deployment-866c9d94b9-jzwgb        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  default                     devopsprojectmicro-deployment-866c9d94b9-qf28v        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  default                     devopsprojectmicro-deployment-866c9d94b9-t6vfm        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10m
  default                     goprom-7bbdc97888-8qgcv                               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         32h
  default                     goprom-7bbdc97888-ktr8q                               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         32h
  default                     goprom-7bbdc97888-wmmvl                               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         32h
  default                     my-nginx-64666fbb86-cp7xk                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
  default                     nginx-demo-554db85f85-42dpt                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         18d
  default                     nginx-demo-554db85f85-4sn9b                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         18d
  default                     nginx-demo-554db85f85-kjn5x                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         18d
  default                     nginx-demo-554db85f85-mwrpv                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         18d
  default                     nginx-demo-554db85f85-s6ptp                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         18d
  default                     nginx-demo-new-6b4d9b5484-42x5c                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3h18m
  default                     nginx-demo-new-6b4d9b5484-dxd9r                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3h18m
  default                     nginx-demo-new-6b4d9b5484-k4js6                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3h20m
  default                     nginx-demo-new-6b4d9b5484-slwjp                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3h18m
  default                     nginx-demo-new-6b4d9b5484-vs8xl                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3h18m
  kube-system                 coredns-5dd5756b68-8hjhv                              100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     18d
  kube-system                 etcd-minikube                                         100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         18d
  kube-system                 kube-apiserver-minikube                               250m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         18d
  kube-system                 kube-controller-manager-minikube                      200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         18d
  kube-system                 kube-proxy-967ql                                      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         18d
  kube-system                 kube-scheduler-minikube                               100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         18d
  kube-system                 storage-provisioner                                   0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         18d
  monitoring                  kube-state-metrics-7589748fd9-sbcpt                   0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
  monitoring                  prometheus-alertmanager-0                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
  monitoring                  prometheus-kube-state-metrics-6b464f5b88-8c7r8        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
  monitoring                  prometheus-prometheus-node-exporter-n89mh             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
  monitoring                  prometheus-prometheus-pushgateway-7857c44f49-fwzws    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
  monitoring                  prometheus-server-8fffdb69d-wvt4r                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         10d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (3%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [  +0.001338] blk_update_request: I/O error, dev sdc, sector 0 op 0x1:(WRITE) flags 0x800 phys_seg 0 prio class 0
[  +0.001346] blk_update_request: I/O error, dev sdc, sector 0 op 0x1:(WRITE) flags 0x800 phys_seg 0 prio class 0
[  +0.000738] blk_update_request: I/O error, dev sdc, sector 1073479680 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0
[  +0.001045] blk_update_request: I/O error, dev sdc, sector 1073479680 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0
[  +0.000916] Buffer I/O error on dev sdc, logical block 134184960, lost sync page write
[  +0.000877] JBD2: Error -5 detected when updating journal superblock for sdc-8.
[  +0.000526] Aborting journal on device sdc-8.
[  +0.000683] blk_update_request: I/O error, dev sdc, sector 1073479680 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0
[  +0.000645] blk_update_request: I/O error, dev sdc, sector 1073479680 op 0x1:(WRITE) flags 0x800 phys_seg 1 prio class 0
[  +0.000944] Buffer I/O error on dev sdc, logical block 134184960, lost sync page write
[  +0.000967] JBD2: Error -5 detected when updating journal superblock for sdc-8.
[  +0.000638] EXT4-fs error (device sdc): ext4_put_super:1196: comm Xwayland: Couldn't clean up the journal
[  +0.001187] blk_update_request: I/O error, dev sdc, sector 0 op 0x1:(WRITE) flags 0x3800 phys_seg 1 prio class 0
[  +0.001231] blk_update_request: I/O error, dev sdc, sector 0 op 0x1:(WRITE) flags 0x3800 phys_seg 1 prio class 0
[  +0.001218] Buffer I/O error on dev sdc, logical block 0, lost sync page write
[  +0.000573] EXT4-fs (sdc): I/O error while writing superblock
[  +0.000553] EXT4-fs (sdc): Remounting filesystem read-only
[  +1.687723] /sbin/ldconfig: 
[  +0.000003] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.035802] WSL (1) ERROR: ConfigApplyWindowsLibPath:2529: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000004]  failed 2
[  +0.013898] FS-Cache: Duplicate cookie detected
[  +0.000689] FS-Cache: O-cookie c=00000011 [p=00000002 fl=222 nc=0 na=1]
[  +0.000816] FS-Cache: O-cookie d=00000000b70b4799{9P.session} n=000000006bc224ca
[  +0.001084] FS-Cache: O-key=[10] '34323934393337373734'
[  +0.000479] FS-Cache: N-cookie c=00000012 [p=00000002 fl=2 nc=0 na=1]
[  +0.000554] FS-Cache: N-cookie d=00000000b70b4799{9P.session} n=0000000007b1b352
[  +0.001098] FS-Cache: N-key=[10] '34323934393337373734'
[  +0.006142] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Paris not found. Is the tzdata package installed?
[  +0.079576] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001023] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000800] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000992] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001625] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000675] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000856] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000693] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.518779] /sbin/ldconfig: 
[  +0.000003] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.022215] 9pnet_virtio: no channels available for device drvfs
[  +0.000696] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.189164] WSL (2) ERROR: UtilCreateProcessAndWait:663: /bin/mount failed with 2
[  +0.001211] WSL (1) ERROR: UtilCreateProcessAndWait:685: /bin/mount failed with status 0xff00

[  +0.001763] WSL (1) ERROR: ConfigMountFsTab:2581: Processing fstab with mount -a failed.
[  +0.002142] WSL (1) ERROR: ConfigApplyWindowsLibPath:2529: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000002]  failed 2
[  +0.023234] 9pnet_virtio: no channels available for device drvfs
[  +0.000936] WSL (1) WARNING: mount: waiting for virtio device drvfs
[  +0.076405] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001080] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000752] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001035] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001591] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001056] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001231] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001067] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.089489] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Paris not found. Is the tzdata package installed?

* 
* ==> etcd [0a9e08cdbaad] <==
* {"level":"info","ts":"2023-12-18T14:50:00.574534Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":19536,"took":"875.992µs","hash":2391377934}
{"level":"info","ts":"2023-12-18T14:50:00.574571Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2391377934,"revision":19536,"compact-revision":19278}
{"level":"info","ts":"2023-12-18T14:55:00.581372Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19790}
{"level":"info","ts":"2023-12-18T14:55:00.582452Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":19790,"took":"779.441µs","hash":3501060743}
{"level":"info","ts":"2023-12-18T14:55:00.582487Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3501060743,"revision":19790,"compact-revision":19536}
{"level":"info","ts":"2023-12-18T15:00:00.588509Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20046}
{"level":"info","ts":"2023-12-18T15:00:00.589472Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":20046,"took":"717.196µs","hash":4192304219}
{"level":"info","ts":"2023-12-18T15:00:00.589516Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4192304219,"revision":20046,"compact-revision":19790}
{"level":"info","ts":"2023-12-18T15:05:00.598851Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20301}
{"level":"info","ts":"2023-12-18T15:05:00.600054Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":20301,"took":"936.153µs","hash":2112632200}
{"level":"info","ts":"2023-12-18T15:05:00.60009Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2112632200,"revision":20301,"compact-revision":20046}
{"level":"info","ts":"2023-12-18T15:10:00.605036Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20561}
{"level":"info","ts":"2023-12-18T15:10:00.606117Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":20561,"took":"827.595µs","hash":857400011}
{"level":"info","ts":"2023-12-18T15:10:00.606156Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":857400011,"revision":20561,"compact-revision":20301}
{"level":"info","ts":"2023-12-18T15:13:03.51045Z","caller":"traceutil/trace.go:171","msg":"trace[1897745168] transaction","detail":"{read_only:false; response_revision:20970; number_of_response:1; }","duration":"100.713838ms","start":"2023-12-18T15:13:03.409656Z","end":"2023-12-18T15:13:03.51037Z","steps":["trace[1897745168] 'process raft request'  (duration: 100.586185ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-18T15:13:03.802861Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"188.782762ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-12-18T15:13:03.803019Z","caller":"traceutil/trace.go:171","msg":"trace[1069603444] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:20970; }","duration":"189.037394ms","start":"2023-12-18T15:13:03.613963Z","end":"2023-12-18T15:13:03.803Z","steps":["trace[1069603444] 'range keys from in-memory index tree'  (duration: 188.727363ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T15:15:00.616083Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20821}
{"level":"info","ts":"2023-12-18T15:15:00.617151Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":20821,"took":"821.774µs","hash":2995243560}
{"level":"info","ts":"2023-12-18T15:15:00.617184Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2995243560,"revision":20821,"compact-revision":20561}
{"level":"info","ts":"2023-12-18T15:19:02.309761Z","caller":"traceutil/trace.go:171","msg":"trace[1245906614] transaction","detail":"{read_only:false; response_revision:21289; number_of_response:1; }","duration":"100.226023ms","start":"2023-12-18T15:19:02.209518Z","end":"2023-12-18T15:19:02.309744Z","steps":["trace[1245906614] 'process raft request'  (duration: 100.110415ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T15:19:02.712051Z","caller":"traceutil/trace.go:171","msg":"trace[1837888944] transaction","detail":"{read_only:false; response_revision:21290; number_of_response:1; }","duration":"102.630817ms","start":"2023-12-18T15:19:02.609406Z","end":"2023-12-18T15:19:02.712037Z","steps":["trace[1837888944] 'process raft request'  (duration: 102.337662ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T15:20:00.624594Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":21071}
{"level":"info","ts":"2023-12-18T15:20:00.626494Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":21071,"took":"1.615491ms","hash":2523090906}
{"level":"info","ts":"2023-12-18T15:20:00.626563Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2523090906,"revision":21071,"compact-revision":20821}
{"level":"info","ts":"2023-12-18T15:25:00.716566Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":21336}
{"level":"info","ts":"2023-12-18T15:25:00.818611Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":21336,"took":"101.603777ms","hash":1755442258}
{"level":"info","ts":"2023-12-18T15:25:00.818655Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1755442258,"revision":21336,"compact-revision":21071}
{"level":"info","ts":"2023-12-18T15:30:00.725529Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":21593}
{"level":"info","ts":"2023-12-18T15:30:00.726374Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":21593,"took":"669.734µs","hash":55051728}
{"level":"info","ts":"2023-12-18T15:30:00.726409Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":55051728,"revision":21593,"compact-revision":21336}
{"level":"info","ts":"2023-12-18T15:30:59.809117Z","caller":"traceutil/trace.go:171","msg":"trace[1903910751] transaction","detail":"{read_only:false; response_revision:21897; number_of_response:1; }","duration":"101.184631ms","start":"2023-12-18T15:30:59.707884Z","end":"2023-12-18T15:30:59.809069Z","steps":["trace[1903910751] 'process raft request'  (duration: 100.964517ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T15:35:00.734604Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":21844}
{"level":"info","ts":"2023-12-18T15:35:00.736283Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":21844,"took":"1.481878ms","hash":2844886756}
{"level":"info","ts":"2023-12-18T15:35:00.736319Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2844886756,"revision":21844,"compact-revision":21593}
{"level":"info","ts":"2023-12-18T15:40:00.745879Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":22101}
{"level":"info","ts":"2023-12-18T15:40:00.74814Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":22101,"took":"2.060992ms","hash":458185260}
{"level":"info","ts":"2023-12-18T15:40:00.74817Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":458185260,"revision":22101,"compact-revision":21844}
{"level":"info","ts":"2023-12-18T15:45:00.757631Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":22357}
{"level":"info","ts":"2023-12-18T15:45:00.75928Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":22357,"took":"1.246432ms","hash":3080507475}
{"level":"info","ts":"2023-12-18T15:45:00.759319Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3080507475,"revision":22357,"compact-revision":22101}
{"level":"info","ts":"2023-12-18T15:46:40.808491Z","caller":"traceutil/trace.go:171","msg":"trace[1636524506] transaction","detail":"{read_only:false; response_revision:22691; number_of_response:1; }","duration":"101.924315ms","start":"2023-12-18T15:46:40.706552Z","end":"2023-12-18T15:46:40.808476Z","steps":["trace[1636524506] 'process raft request'  (duration: 101.828638ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T15:50:00.765549Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":22606}
{"level":"info","ts":"2023-12-18T15:50:00.766627Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":22606,"took":"826.113µs","hash":514403675}
{"level":"info","ts":"2023-12-18T15:50:00.766663Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":514403675,"revision":22606,"compact-revision":22357}
{"level":"info","ts":"2023-12-18T15:54:14.60719Z","caller":"traceutil/trace.go:171","msg":"trace[1942979580] transaction","detail":"{read_only:false; response_revision:23066; number_of_response:1; }","duration":"101.940519ms","start":"2023-12-18T15:54:14.505234Z","end":"2023-12-18T15:54:14.607174Z","steps":["trace[1942979580] 'process raft request'  (duration: 99.468199ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T15:55:00.772281Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":22856}
{"level":"info","ts":"2023-12-18T15:55:00.773523Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":22856,"took":"1.052176ms","hash":3886704619}
{"level":"info","ts":"2023-12-18T15:55:00.773561Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3886704619,"revision":22856,"compact-revision":22606}
{"level":"info","ts":"2023-12-18T15:55:37.7983Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2023-12-18T15:55:37.799369Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2023-12-18T15:55:37.897202Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2023-12-18T15:55:37.898444Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
WARNING: 2023/12/18 15:55:37 [core] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"warn","ts":"2023-12-18T15:55:38.006125Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2023-12-18T15:55:38.006221Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2023-12-18T15:55:38.097079Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-12-18T15:55:38.298582Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-12-18T15:55:38.302209Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-12-18T15:55:38.302248Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [643eb0a8cf4c] <==
* {"level":"info","ts":"2023-12-18T19:10:56.131609Z","caller":"traceutil/trace.go:171","msg":"trace[1109260379] range","detail":"{range_begin:/registry/pods/default/devopsprojectmicro-deployment-866c9d94b9-t6vfm; range_end:; response_count:1; response_revision:33373; }","duration":"109.784885ms","start":"2023-12-18T19:10:56.021806Z","end":"2023-12-18T19:10:56.13159Z","steps":["trace[1109260379] 'range keys from in-memory index tree'  (duration: 109.553231ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:10:56.819856Z","caller":"traceutil/trace.go:171","msg":"trace[681335801] transaction","detail":"{read_only:false; response_revision:33376; number_of_response:1; }","duration":"297.128803ms","start":"2023-12-18T19:10:56.522689Z","end":"2023-12-18T19:10:56.819818Z","steps":["trace[681335801] 'process raft request'  (duration: 296.501781ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:11:18.625472Z","caller":"traceutil/trace.go:171","msg":"trace[1696252928] transaction","detail":"{read_only:false; response_revision:33408; number_of_response:1; }","duration":"103.663026ms","start":"2023-12-18T19:11:18.521788Z","end":"2023-12-18T19:11:18.625451Z","steps":["trace[1696252928] 'process raft request'  (duration: 103.491927ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:11:41.323113Z","caller":"traceutil/trace.go:171","msg":"trace[1510942134] transaction","detail":"{read_only:false; response_revision:33428; number_of_response:1; }","duration":"100.641755ms","start":"2023-12-18T19:11:41.222441Z","end":"2023-12-18T19:11:41.323083Z","steps":["trace[1510942134] 'process raft request'  (duration: 100.465262ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:11:51.819165Z","caller":"traceutil/trace.go:171","msg":"trace[223098610] transaction","detail":"{read_only:false; response_revision:33436; number_of_response:1; }","duration":"199.494829ms","start":"2023-12-18T19:11:51.619645Z","end":"2023-12-18T19:11:51.81914Z","steps":["trace[223098610] 'process raft request'  (duration: 199.329256ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-18T19:11:52.019024Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"198.643335ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-12-18T19:11:52.019086Z","caller":"traceutil/trace.go:171","msg":"trace[238767521] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:33436; }","duration":"198.715177ms","start":"2023-12-18T19:11:51.820358Z","end":"2023-12-18T19:11:52.019073Z","steps":["trace[238767521] 'range keys from in-memory index tree'  (duration: 198.555437ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-18T19:11:55.019056Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.276364ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128025898751078630 username:\"kube-apiserver-etcd-client\" auth_revision:1 > lease_grant:<ttl:15-second id:70cc8c7da43bb0e5>","response":"size:42"}
{"level":"info","ts":"2023-12-18T19:11:55.121589Z","caller":"traceutil/trace.go:171","msg":"trace[1626736924] transaction","detail":"{read_only:false; response_revision:33440; number_of_response:1; }","duration":"100.630927ms","start":"2023-12-18T19:11:55.020914Z","end":"2023-12-18T19:11:55.121544Z","steps":["trace[1626736924] 'process raft request'  (duration: 97.844923ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:11:58.799902Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":32983}
{"level":"info","ts":"2023-12-18T19:11:58.801168Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":32983,"took":"1.024494ms","hash":586730492}
{"level":"info","ts":"2023-12-18T19:11:58.801207Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":586730492,"revision":32983,"compact-revision":32736}
{"level":"info","ts":"2023-12-18T19:12:15.021648Z","caller":"traceutil/trace.go:171","msg":"trace[1300006139] transaction","detail":"{read_only:false; response_revision:33461; number_of_response:1; }","duration":"186.43193ms","start":"2023-12-18T19:12:14.835179Z","end":"2023-12-18T19:12:15.021611Z","steps":["trace[1300006139] 'process raft request'  (duration: 183.775158ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:12:47.529901Z","caller":"traceutil/trace.go:171","msg":"trace[2134439860] transaction","detail":"{read_only:false; response_revision:33493; number_of_response:1; }","duration":"107.07488ms","start":"2023-12-18T19:12:47.422809Z","end":"2023-12-18T19:12:47.529884Z","steps":["trace[2134439860] 'process raft request'  (duration: 106.700426ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:12:53.425341Z","caller":"traceutil/trace.go:171","msg":"trace[707041309] transaction","detail":"{read_only:false; response_revision:33496; number_of_response:1; }","duration":"103.255029ms","start":"2023-12-18T19:12:53.322061Z","end":"2023-12-18T19:12:53.425316Z","steps":["trace[707041309] 'process raft request'  (duration: 102.824877ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-18T19:13:30.929912Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"108.510147ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-12-18T19:13:30.929993Z","caller":"traceutil/trace.go:171","msg":"trace[531682490] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:33531; }","duration":"108.596969ms","start":"2023-12-18T19:13:30.821378Z","end":"2023-12-18T19:13:30.929975Z","steps":["trace[531682490] 'range keys from in-memory index tree'  (duration: 108.422909ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:13:53.918781Z","caller":"traceutil/trace.go:171","msg":"trace[2062190100] transaction","detail":"{read_only:false; response_revision:33552; number_of_response:1; }","duration":"300.169216ms","start":"2023-12-18T19:13:53.618573Z","end":"2023-12-18T19:13:53.918742Z","steps":["trace[2062190100] 'process raft request'  (duration: 299.93515ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-18T19:13:53.921721Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-18T19:13:53.61854Z","time spent":"300.365912ms","remote":"127.0.0.1:56948","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:33551 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2023-12-18T19:13:55.82197Z","caller":"traceutil/trace.go:171","msg":"trace[862606475] transaction","detail":"{read_only:false; response_revision:33555; number_of_response:1; }","duration":"103.474127ms","start":"2023-12-18T19:13:55.718435Z","end":"2023-12-18T19:13:55.821909Z","steps":["trace[862606475] 'process raft request'  (duration: 103.361684ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:14:05.144008Z","caller":"traceutil/trace.go:171","msg":"trace[379277208] linearizableReadLoop","detail":"{readStateIndex:41263; appliedIndex:41262; }","duration":"119.027335ms","start":"2023-12-18T19:14:05.024965Z","end":"2023-12-18T19:14:05.143992Z","steps":["trace[379277208] 'read index received'  (duration: 95.309764ms)","trace[379277208] 'applied index is now lower than readState.Index'  (duration: 23.71704ms)"],"step_count":2}
{"level":"warn","ts":"2023-12-18T19:14:05.14414Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"119.183183ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/\" range_end:\"/registry/masterleases0\" ","response":"range_response_count:1 size:136"}
{"level":"info","ts":"2023-12-18T19:14:05.144172Z","caller":"traceutil/trace.go:171","msg":"trace[585327407] range","detail":"{range_begin:/registry/masterleases/; range_end:/registry/masterleases0; response_count:1; response_revision:33563; }","duration":"119.229245ms","start":"2023-12-18T19:14:05.024935Z","end":"2023-12-18T19:14:05.144164Z","steps":["trace[585327407] 'agreement among raft nodes before linearized reading'  (duration: 119.123666ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:14:05.144165Z","caller":"traceutil/trace.go:171","msg":"trace[961123536] transaction","detail":"{read_only:false; response_revision:33563; number_of_response:1; }","duration":"121.05786ms","start":"2023-12-18T19:14:05.023092Z","end":"2023-12-18T19:14:05.14415Z","steps":["trace[961123536] 'process raft request'  (duration: 97.299851ms)","trace[961123536] 'compare'  (duration: 23.461929ms)"],"step_count":2}
{"level":"info","ts":"2023-12-18T19:14:27.24675Z","caller":"traceutil/trace.go:171","msg":"trace[1569482730] transaction","detail":"{read_only:false; response_revision:33580; number_of_response:1; }","duration":"189.857186ms","start":"2023-12-18T19:14:27.056844Z","end":"2023-12-18T19:14:27.246702Z","steps":["trace[1569482730] 'process raft request'  (duration: 189.59381ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:14:27.358722Z","caller":"traceutil/trace.go:171","msg":"trace[334343315] transaction","detail":"{read_only:false; response_revision:33581; number_of_response:1; }","duration":"301.699875ms","start":"2023-12-18T19:14:27.056994Z","end":"2023-12-18T19:14:27.358694Z","steps":["trace[334343315] 'process raft request'  (duration: 301.539148ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-18T19:14:27.358942Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-18T19:14:27.056966Z","time spent":"301.891533ms","remote":"127.0.0.1:56948","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:33577 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2023-12-18T19:15:33.026544Z","caller":"traceutil/trace.go:171","msg":"trace[1475409897] transaction","detail":"{read_only:false; response_revision:33641; number_of_response:1; }","duration":"100.117028ms","start":"2023-12-18T19:15:32.926356Z","end":"2023-12-18T19:15:33.026473Z","steps":["trace[1475409897] 'process raft request'  (duration: 99.940712ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:15:53.82899Z","caller":"traceutil/trace.go:171","msg":"trace[94882869] transaction","detail":"{read_only:false; response_revision:33660; number_of_response:1; }","duration":"106.607111ms","start":"2023-12-18T19:15:53.722359Z","end":"2023-12-18T19:15:53.828966Z","steps":["trace[94882869] 'process raft request'  (duration: 106.463129ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:16:18.414095Z","caller":"traceutil/trace.go:171","msg":"trace[497230125] transaction","detail":"{read_only:false; response_revision:33682; number_of_response:1; }","duration":"195.084567ms","start":"2023-12-18T19:16:18.218965Z","end":"2023-12-18T19:16:18.41405Z","steps":["trace[497230125] 'process raft request'  (duration: 194.838008ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:16:18.818546Z","caller":"traceutil/trace.go:171","msg":"trace[2114409143] transaction","detail":"{read_only:false; response_revision:33683; number_of_response:1; }","duration":"196.437076ms","start":"2023-12-18T19:16:18.622082Z","end":"2023-12-18T19:16:18.818519Z","steps":["trace[2114409143] 'process raft request'  (duration: 196.275762ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:16:30.212868Z","caller":"traceutil/trace.go:171","msg":"trace[1755234893] transaction","detail":"{read_only:false; response_revision:33692; number_of_response:1; }","duration":"183.041882ms","start":"2023-12-18T19:16:30.0298Z","end":"2023-12-18T19:16:30.212842Z","steps":["trace[1755234893] 'process raft request'  (duration: 182.521097ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:16:41.231714Z","caller":"traceutil/trace.go:171","msg":"trace[482170238] transaction","detail":"{read_only:false; response_revision:33701; number_of_response:1; }","duration":"111.925105ms","start":"2023-12-18T19:16:41.119746Z","end":"2023-12-18T19:16:41.231671Z","steps":["trace[482170238] 'process raft request'  (duration: 111.805343ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:16:51.812127Z","caller":"traceutil/trace.go:171","msg":"trace[109259877] transaction","detail":"{read_only:false; response_revision:33709; number_of_response:1; }","duration":"194.687685ms","start":"2023-12-18T19:16:51.617383Z","end":"2023-12-18T19:16:51.812071Z","steps":["trace[109259877] 'process raft request'  (duration: 193.891756ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:16:58.869473Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":33443}
{"level":"info","ts":"2023-12-18T19:16:58.871759Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":33443,"took":"1.892756ms","hash":3804383434}
{"level":"info","ts":"2023-12-18T19:16:58.871832Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3804383434,"revision":33443,"compact-revision":32983}
{"level":"info","ts":"2023-12-18T19:17:14.911548Z","caller":"traceutil/trace.go:171","msg":"trace[605439896] linearizableReadLoop","detail":"{readStateIndex:41468; appliedIndex:41467; }","duration":"184.580564ms","start":"2023-12-18T19:17:14.72692Z","end":"2023-12-18T19:17:14.9115Z","steps":["trace[605439896] 'read index received'  (duration: 184.141278ms)","trace[605439896] 'applied index is now lower than readState.Index'  (duration: 437.88µs)"],"step_count":2}
{"level":"info","ts":"2023-12-18T19:17:14.912271Z","caller":"traceutil/trace.go:171","msg":"trace[1803677989] transaction","detail":"{read_only:false; response_revision:33728; number_of_response:1; }","duration":"199.070479ms","start":"2023-12-18T19:17:14.713156Z","end":"2023-12-18T19:17:14.912226Z","steps":["trace[1803677989] 'process raft request'  (duration: 198.03086ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-18T19:17:15.011456Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"185.18293ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-12-18T19:17:15.011594Z","caller":"traceutil/trace.go:171","msg":"trace[610841142] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:33728; }","duration":"284.690714ms","start":"2023-12-18T19:17:14.726863Z","end":"2023-12-18T19:17:15.011554Z","steps":["trace[610841142] 'agreement among raft nodes before linearized reading'  (duration: 185.042333ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-18T19:17:15.126702Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"115.345968ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128025898751080048 username:\"kube-apiserver-etcd-client\" auth_revision:1 > lease_grant:<ttl:15-second id:70cc8c7da43bb66f>","response":"size:42"}
{"level":"info","ts":"2023-12-18T19:17:15.126855Z","caller":"traceutil/trace.go:171","msg":"trace[496914384] linearizableReadLoop","detail":"{readStateIndex:41469; appliedIndex:41468; }","duration":"112.282557ms","start":"2023-12-18T19:17:15.01455Z","end":"2023-12-18T19:17:15.126833Z","steps":["trace[496914384] 'read index received'  (duration: 76.114µs)","trace[496914384] 'applied index is now lower than readState.Index'  (duration: 112.204211ms)"],"step_count":2}
{"level":"warn","ts":"2023-12-18T19:17:15.126954Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"112.43359ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-12-18T19:17:15.126985Z","caller":"traceutil/trace.go:171","msg":"trace[1762371469] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:33728; }","duration":"112.475125ms","start":"2023-12-18T19:17:15.014497Z","end":"2023-12-18T19:17:15.126972Z","steps":["trace[1762371469] 'agreement among raft nodes before linearized reading'  (duration: 112.397134ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:17:53.813216Z","caller":"traceutil/trace.go:171","msg":"trace[1233551843] transaction","detail":"{read_only:false; response_revision:33759; number_of_response:1; }","duration":"103.105857ms","start":"2023-12-18T19:17:53.710089Z","end":"2023-12-18T19:17:53.813195Z","steps":["trace[1233551843] 'process raft request'  (duration: 103.000047ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:17:58.111939Z","caller":"traceutil/trace.go:171","msg":"trace[575834854] transaction","detail":"{read_only:false; response_revision:33762; number_of_response:1; }","duration":"192.680486ms","start":"2023-12-18T19:17:57.91922Z","end":"2023-12-18T19:17:58.1119Z","steps":["trace[575834854] 'process raft request'  (duration: 192.452399ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:18:08.517556Z","caller":"traceutil/trace.go:171","msg":"trace[1953134684] transaction","detail":"{read_only:false; response_revision:33770; number_of_response:1; }","duration":"164.233582ms","start":"2023-12-18T19:18:08.353298Z","end":"2023-12-18T19:18:08.517531Z","steps":["trace[1953134684] 'process raft request'  (duration: 164.063282ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:18:09.223766Z","caller":"traceutil/trace.go:171","msg":"trace[634594751] transaction","detail":"{read_only:false; response_revision:33771; number_of_response:1; }","duration":"105.530736ms","start":"2023-12-18T19:18:09.118202Z","end":"2023-12-18T19:18:09.223733Z","steps":["trace[634594751] 'process raft request'  (duration: 105.32712ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:18:31.510277Z","caller":"traceutil/trace.go:171","msg":"trace[716518268] linearizableReadLoop","detail":"{readStateIndex:41547; appliedIndex:41546; }","duration":"100.032833ms","start":"2023-12-18T19:18:31.410219Z","end":"2023-12-18T19:18:31.510252Z","steps":["trace[716518268] 'read index received'  (duration: 99.819049ms)","trace[716518268] 'applied index is now lower than readState.Index'  (duration: 213.141µs)"],"step_count":2}
{"level":"info","ts":"2023-12-18T19:18:31.510635Z","caller":"traceutil/trace.go:171","msg":"trace[1036233032] transaction","detail":"{read_only:false; response_revision:33790; number_of_response:1; }","duration":"192.910209ms","start":"2023-12-18T19:18:31.317711Z","end":"2023-12-18T19:18:31.510621Z","steps":["trace[1036233032] 'process raft request'  (duration: 192.41454ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-18T19:18:31.610365Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.657838ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2023-12-18T19:18:31.610504Z","caller":"traceutil/trace.go:171","msg":"trace[990847071] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:33790; }","duration":"200.280966ms","start":"2023-12-18T19:18:31.410192Z","end":"2023-12-18T19:18:31.610473Z","steps":["trace[990847071] 'agreement among raft nodes before linearized reading'  (duration: 100.620326ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:18:31.622653Z","caller":"traceutil/trace.go:171","msg":"trace[785504416] transaction","detail":"{read_only:false; response_revision:33791; number_of_response:1; }","duration":"106.972031ms","start":"2023-12-18T19:18:31.515657Z","end":"2023-12-18T19:18:31.622629Z","steps":["trace[785504416] 'process raft request'  (duration: 106.833636ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:18:31.823335Z","caller":"traceutil/trace.go:171","msg":"trace[1465791793] linearizableReadLoop","detail":"{readStateIndex:41549; appliedIndex:41548; }","duration":"107.527169ms","start":"2023-12-18T19:18:31.715787Z","end":"2023-12-18T19:18:31.823315Z","steps":["trace[1465791793] 'read index received'  (duration: 94.022767ms)","trace[1465791793] 'applied index is now lower than readState.Index'  (duration: 13.503381ms)"],"step_count":2}
{"level":"warn","ts":"2023-12-18T19:18:31.823442Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"107.636031ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-12-18T19:18:31.823456Z","caller":"traceutil/trace.go:171","msg":"trace[479501326] transaction","detail":"{read_only:false; response_revision:33792; number_of_response:1; }","duration":"204.900496ms","start":"2023-12-18T19:18:31.618525Z","end":"2023-12-18T19:18:31.823425Z","steps":["trace[479501326] 'process raft request'  (duration: 191.32652ms)"],"step_count":1}
{"level":"info","ts":"2023-12-18T19:18:31.823482Z","caller":"traceutil/trace.go:171","msg":"trace[1250439252] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:33792; }","duration":"107.731027ms","start":"2023-12-18T19:18:31.715744Z","end":"2023-12-18T19:18:31.823475Z","steps":["trace[1250439252] 'agreement among raft nodes before linearized reading'  (duration: 107.650941ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-18T19:18:53.910003Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"190.583834ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-12-18T19:18:53.910127Z","caller":"traceutil/trace.go:171","msg":"trace[690173259] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:33814; }","duration":"190.732813ms","start":"2023-12-18T19:18:53.719372Z","end":"2023-12-18T19:18:53.910104Z","steps":["trace[690173259] 'range keys from in-memory index tree'  (duration: 190.433579ms)"],"step_count":1}

* 
* ==> kernel <==
*  19:18:57 up  9:25,  0 users,  load average: 0.60, 0.62, 0.73
Linux minikube 5.15.133.1-microsoft-standard-WSL2 #1 SMP Thu Oct 5 21:02:42 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [7b3e31c874c4] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1218 15:55:47.566491       1 logging.go:59] [core] [Channel #64 SubChannel #65] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1218 15:55:47.573139       1 logging.go:59] [core] [Channel #115 SubChannel #116] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1218 15:55:47.621038       1 logging.go:59] [core] [Channel #88 SubChannel #89] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1218 15:55:47.686996       1 logging.go:59] [core] [Channel #160 SubChannel #161] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1218 15:55:47.703776       1 logging.go:59] [core] [Channel #46 SubChannel #47] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1218 15:55:47.750401       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1218 15:55:47.801724       1 logging.go:59] [core] [Channel #169 SubChannel #170] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-apiserver [a3da884807b5] <==
* I1218 17:41:08.331743       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 17:41:08.331822       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 17:46:08.332874       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 17:46:08.333002       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 17:51:59.423140       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 17:51:59.423329       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 17:56:59.429604       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 17:56:59.429792       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 17:56:59.430004       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:01:59.506051       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:01:59.506182       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:06:59.506401       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:11:59.506747       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:11:59.506942       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:11:59.507089       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:16:59.506028       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:16:59.506176       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:16:59.506343       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:21:59.506097       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:21:59.506164       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:26:59.506663       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:26:59.506734       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:31:59.506078       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:31:59.506190       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:31:59.506464       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:36:59.522041       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:36:59.522162       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:36:59.522268       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:41:58.559858       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:41:58.560002       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:46:58.554161       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:46:58.554243       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:46:58.554318       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:51:54.947640       1 trace.go:236] Trace[1077464031]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (18-Dec-2023 18:51:54.241) (total time: 608ms):
Trace[1077464031]: ---"Txn call completed" 599ms (18:51:54.849)
Trace[1077464031]: [608.483909ms] [608.483909ms] END
I1218 18:51:58.550420       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:51:58.550515       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:56:58.546575       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:56:58.546905       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 18:57:27.147332       1 trace.go:236] Trace[791548946]: "List" accept:application/json,audit-id:a16becd2-1bcf-4581-acb9-553088327343,client:10.244.0.146,protocol:HTTP/2.0,resource:applications,scope:cluster,url:/apis/argoproj.io/v1alpha1/applications,user-agent:argocd-application-controller/v0.0.0 (linux/amd64) kubernetes/$Format,verb:LIST (18-Dec-2023 18:57:26.641) (total time: 505ms):
Trace[791548946]: ---"Writing http response done" count:3 501ms (18:57:27.147)
Trace[791548946]: [505.750634ms] [505.750634ms] END
I1218 19:01:58.546977       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 19:01:58.547099       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 19:06:58.531849       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 19:06:58.532145       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 19:06:58.532272       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 19:08:45.106078       1 alloc.go:330] "allocated clusterIPs" service="default/devopsprojectmicro-nodeport" clusterIPs={"IPv4":"10.110.72.154"}
I1218 19:08:57.893713       1 alloc.go:330] "allocated clusterIPs" service="default/devopsprojectmicro-loadbalancer" clusterIPs={"IPv4":"10.110.196.5"}
I1218 19:09:28.620877       1 trace.go:236] Trace[1823420088]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:2001178b-0db7-4bc4-b469-d0da156b80ae,client:192.168.49.2,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/default/pods/devopsprojectmicro-deployment-866c9d94b9-jzwgb/status,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PATCH (18-Dec-2023 19:09:28.034) (total time: 586ms):
Trace[1823420088]: ---"Object stored in database" 489ms (19:09:28.526)
Trace[1823420088]: ---"Writing http response done" 94ms (19:09:28.620)
Trace[1823420088]: [586.052847ms] [586.052847ms] END
I1218 19:11:58.528457       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 19:11:58.528810       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 19:11:58.529032       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 19:16:58.521278       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 19:16:58.521507       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1218 19:16:58.521649       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager

* 
* ==> kube-controller-manager [b4022bf23bb6] <==
* I1218 19:09:11.954353       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="292.379µs"
I1218 19:09:17.952388       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="447.667µs"
I1218 19:09:21.955318       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="143.623µs"
I1218 19:09:23.951224       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="57.429µs"
I1218 19:09:24.958670       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="49.571µs"
I1218 19:09:28.624883       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="80.531µs"
I1218 19:09:31.960113       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="2.552245ms"
I1218 19:09:33.010730       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="773.339µs"
I1218 19:09:36.950260       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="725.676µs"
I1218 19:09:39.324062       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="141.39µs"
I1218 19:09:40.958671       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="2.601298ms"
I1218 19:09:43.952205       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="799.177µs"
I1218 19:09:48.029720       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="93.389µs"
I1218 19:09:58.012124       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="534.079µs"
I1218 19:09:59.025787       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="1.557819ms"
I1218 19:10:01.421173       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="179.25µs"
I1218 19:10:03.014320       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="75.825µs"
I1218 19:10:07.024000       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="1.404379ms"
I1218 19:10:12.951906       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="71.274µs"
I1218 19:10:14.010731       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="110.668µs"
I1218 19:10:14.954696       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="789.72µs"
I1218 19:10:20.968183       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="50.958µs"
I1218 19:10:32.023135       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="85.764µs"
I1218 19:10:43.963887       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="121.18µs"
I1218 19:10:47.020839       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="65.285µs"
I1218 19:10:51.016627       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="58.766µs"
I1218 19:10:54.951018       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="2.529564ms"
I1218 19:10:56.329083       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="1.659376ms"
I1218 19:11:05.022099       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="68.712µs"
I1218 19:11:05.035608       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="52.978µs"
I1218 19:11:06.956783       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="63.103µs"
I1218 19:11:19.958695       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="200.352µs"
I1218 19:12:01.026850       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="1.64557ms"
I1218 19:12:11.948505       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="77.323µs"
I1218 19:12:21.023633       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="127.964µs"
I1218 19:12:26.016059       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="66.158µs"
I1218 19:12:30.988231       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="107.739µs"
I1218 19:12:34.952589       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="64.435µs"
I1218 19:12:38.011837       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="78.469µs"
I1218 19:12:40.009507       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="82.108µs"
I1218 19:12:46.016540       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="300.03µs"
I1218 19:12:55.016471       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="52.762µs"
I1218 19:13:26.031344       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="160.889µs"
I1218 19:13:39.019158       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="1.00395ms"
I1218 19:13:46.015948       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="96.266µs"
I1218 19:14:01.008093       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="495.023µs"
I1218 19:14:41.974922       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="126.977µs"
I1218 19:14:54.956292       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="2.462939ms"
I1218 19:14:56.024162       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="81.812µs"
I1218 19:15:07.013614       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="75.355µs"
I1218 19:15:17.016207       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="118.359µs"
I1218 19:15:19.952800       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="118.819µs"
I1218 19:15:24.001910       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="129.464µs"
I1218 19:15:32.018490       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="78.71µs"
I1218 19:15:32.032950       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="50.464µs"
I1218 19:15:36.021170       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="157.958µs"
I1218 19:15:38.013283       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="135.337µs"
I1218 19:15:48.955204       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/devopsprojectmicro-deployment-866c9d94b9" duration="139.371µs"
I1218 19:18:34.942494       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="98.299µs"
I1218 19:18:46.942364       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="75.048µs"

* 
* ==> kube-controller-manager [d60853a3eb04] <==
* I1218 15:07:52.832152       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="103.656µs"
I1218 15:08:08.004115       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="188.241µs"
I1218 15:08:27.837383       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="118.735µs"
I1218 15:08:36.837269       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="65.422µs"
I1218 15:08:38.829917       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="64.18µs"
I1218 15:08:48.836513       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="51.26µs"
I1218 15:13:10.834667       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="101.838µs"
I1218 15:13:22.836277       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="88.318µs"
I1218 15:13:35.923872       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="100.142µs"
I1218 15:13:46.834626       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="88.948µs"
I1218 15:13:58.829687       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="82.332µs"
I1218 15:14:13.834045       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="84.973µs"
I1218 15:18:22.837404       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="151.627µs"
I1218 15:18:33.833900       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="120.903µs"
I1218 15:18:45.835907       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="111.254µs"
I1218 15:18:58.829367       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="70.147µs"
I1218 15:19:01.837107       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="118.076µs"
I1218 15:19:13.834480       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="62.4µs"
I1218 15:23:32.830658       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="788.73µs"
I1218 15:23:46.833776       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="67.197µs"
I1218 15:23:54.832569       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="98.236µs"
I1218 15:24:03.830621       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="68.983µs"
I1218 15:24:06.832881       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="72.795µs"
I1218 15:24:14.836700       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="74.617µs"
I1218 15:28:49.829578       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="204.183µs"
I1218 15:28:59.828585       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="86.338µs"
I1218 15:29:03.831816       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="80.819µs"
I1218 15:29:11.826810       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="89.215µs"
I1218 15:29:28.829412       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="120.531µs"
I1218 15:29:40.829573       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="90.032µs"
I1218 15:33:56.835315       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="92.806µs"
I1218 15:34:11.836050       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="141.148µs"
I1218 15:34:12.828277       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="221.923µs"
I1218 15:34:26.830730       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="177.861µs"
I1218 15:34:31.832292       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="128.754µs"
I1218 15:34:46.829316       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="52.744µs"
I1218 15:39:03.826556       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="124.185µs"
I1218 15:39:15.830007       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="102.003µs"
I1218 15:39:20.830015       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="92.287µs"
I1218 15:39:34.828077       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="78.317µs"
I1218 15:39:44.828067       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="69.603µs"
I1218 15:39:57.828036       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="77.507µs"
I1218 15:44:15.835507       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="540.516µs"
I1218 15:44:28.828985       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="69.907µs"
I1218 15:44:30.830231       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="55.076µs"
I1218 15:44:45.828348       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="70.433µs"
I1218 15:44:51.830602       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="60.706µs"
I1218 15:45:04.825331       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="54.141µs"
I1218 15:49:19.828206       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="103.982µs"
I1218 15:49:34.826349       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="87.313µs"
I1218 15:49:37.827881       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="64.734µs"
I1218 15:49:50.830056       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="53.52µs"
I1218 15:50:02.996689       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="199.455µs"
I1218 15:50:16.823636       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="128.427µs"
I1218 15:54:26.826639       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="238.245µs"
I1218 15:54:40.824409       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="69.409µs"
I1218 15:54:47.826343       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="87.013µs"
I1218 15:54:58.823084       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="92.27µs"
I1218 15:55:10.824291       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="84.209µs"
I1218 15:55:24.823142       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/goprom-7bbdc97888" duration="77.088µs"

* 
* ==> kube-proxy [6b6ef07eb900] <==
* I1218 15:55:56.908715       1 server_others.go:69] "Using iptables proxy"
E1218 15:55:56.914359       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
I1218 15:56:08.099469       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1218 15:56:08.604390       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1218 15:56:08.606945       1 server_others.go:152] "Using iptables Proxier"
I1218 15:56:08.607028       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I1218 15:56:08.607042       1 server_others.go:438] "Defaulting to no-op detect-local"
I1218 15:56:08.607483       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1218 15:56:08.608525       1 server.go:846] "Version info" version="v1.28.3"
I1218 15:56:08.608572       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1218 15:56:08.616940       1 config.go:188] "Starting service config controller"
I1218 15:56:08.617084       1 config.go:315] "Starting node config controller"
I1218 15:56:08.617361       1 shared_informer.go:311] Waiting for caches to sync for service config
I1218 15:56:08.616978       1 config.go:97] "Starting endpoint slice config controller"
I1218 15:56:08.617406       1 shared_informer.go:311] Waiting for caches to sync for node config
I1218 15:56:08.617437       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1218 15:56:08.718343       1 shared_informer.go:318] Caches are synced for node config
I1218 15:56:08.718358       1 shared_informer.go:318] Caches are synced for endpoint slice config
I1218 15:56:08.718372       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-proxy [f49676c68eba] <==
* I1218 14:15:04.230983       1 server_others.go:69] "Using iptables proxy"
I1218 14:15:04.331152       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1218 14:15:04.512485       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1218 14:15:04.514937       1 server_others.go:152] "Using iptables Proxier"
I1218 14:15:04.515002       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I1218 14:15:04.515013       1 server_others.go:438] "Defaulting to no-op detect-local"
I1218 14:15:04.515082       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1218 14:15:04.515407       1 server.go:846] "Version info" version="v1.28.3"
I1218 14:15:04.515446       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1218 14:15:04.520157       1 config.go:97] "Starting endpoint slice config controller"
I1218 14:15:04.520194       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1218 14:15:04.520217       1 config.go:188] "Starting service config controller"
I1218 14:15:04.520221       1 shared_informer.go:311] Waiting for caches to sync for service config
I1218 14:15:04.518568       1 config.go:315] "Starting node config controller"
I1218 14:15:04.520398       1 shared_informer.go:311] Waiting for caches to sync for node config
I1218 14:15:04.620287       1 shared_informer.go:318] Caches are synced for endpoint slice config
I1218 14:15:04.620320       1 shared_informer.go:318] Caches are synced for service config
I1218 14:15:04.620787       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-scheduler [a6a599c0f82c] <==
* I1218 14:14:59.938101       1 serving.go:348] Generated self-signed cert in-memory
W1218 14:15:01.572421       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1218 14:15:01.572466       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1218 14:15:01.572475       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W1218 14:15:01.572481       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1218 14:15:01.826577       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I1218 14:15:01.826639       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1218 14:15:01.828462       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1218 14:15:01.828588       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1218 14:15:01.828597       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1218 14:15:01.828607       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1218 14:15:02.013079       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
E1218 15:55:37.907686       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kube-scheduler [dedb6961b8d0] <==
* I1218 15:56:02.321790       1 serving.go:348] Generated self-signed cert in-memory
W1218 15:56:07.805512       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1218 15:56:07.805628       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1218 15:56:07.805640       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W1218 15:56:07.805649       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1218 15:56:08.010747       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I1218 15:56:08.010800       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1218 15:56:08.014466       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1218 15:56:08.014916       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1218 15:56:08.015004       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1218 15:56:08.015023       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1218 15:56:08.196132       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Dec 18 19:17:49 minikube kubelet[77234]: E1218 19:17:49.539910   77234 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="881a69ac7921557dd4b472eb13be8d81e239816fd7d2e5c91b42a246491db1bb"
Dec 18 19:17:49 minikube kubelet[77234]: E1218 19:17:49.540007   77234 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/monitoring_kube-state-metrics-7589748fd9-sbcpt_3db27fd1-475e-4229-a921-88a928c0f6fc/kube-state-metrics/6.log\": failed to reopen container log \"881a69ac7921557dd4b472eb13be8d81e239816fd7d2e5c91b42a246491db1bb\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/monitoring_kube-state-metrics-7589748fd9-sbcpt_3db27fd1-475e-4229-a921-88a928c0f6fc/kube-state-metrics/6.log" containerID="881a69ac7921557dd4b472eb13be8d81e239816fd7d2e5c91b42a246491db1bb"
Dec 18 19:17:49 minikube kubelet[77234]: E1218 19:17:49.930970   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"goprom\" with ImagePullBackOff: \"Back-off pulling image \\\"username/goprom\\\"\"" pod="default/goprom-7bbdc97888-wmmvl" podUID="3c2f7617-13f9-417e-82bc-427246fe3360"
Dec 18 19:17:51 minikube kubelet[77234]: E1218 19:17:51.932176   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"goprom\" with ImagePullBackOff: \"Back-off pulling image \\\"username/goprom\\\"\"" pod="default/goprom-7bbdc97888-8qgcv" podUID="30f44d01-fe43-40be-846c-e3e78ae1cd93"
Dec 18 19:17:51 minikube kubelet[77234]: E1218 19:17:51.932228   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-fv75s" podUID="548b1e30-613d-4b50-bf50-ed293b0cee27"
Dec 18 19:17:54 minikube kubelet[77234]: E1218 19:17:54.931853   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-t6vfm" podUID="7585646c-45f1-490d-aea0-d5f65a1e9ed6"
Dec 18 19:17:55 minikube kubelet[77234]: E1218 19:17:55.931427   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-qf28v" podUID="228194bd-47d5-4224-9089-1d64fd9be41f"
Dec 18 19:17:59 minikube kubelet[77234]: E1218 19:17:59.597199   77234 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="881a69ac7921557dd4b472eb13be8d81e239816fd7d2e5c91b42a246491db1bb"
Dec 18 19:17:59 minikube kubelet[77234]: E1218 19:17:59.597267   77234 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/monitoring_kube-state-metrics-7589748fd9-sbcpt_3db27fd1-475e-4229-a921-88a928c0f6fc/kube-state-metrics/6.log\": failed to reopen container log \"881a69ac7921557dd4b472eb13be8d81e239816fd7d2e5c91b42a246491db1bb\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/monitoring_kube-state-metrics-7589748fd9-sbcpt_3db27fd1-475e-4229-a921-88a928c0f6fc/kube-state-metrics/6.log" containerID="881a69ac7921557dd4b472eb13be8d81e239816fd7d2e5c91b42a246491db1bb"
Dec 18 19:17:59 minikube kubelet[77234]: E1218 19:17:59.931057   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-bhqv2" podUID="99491fcb-0a27-4f7b-9697-24631450984e"
Dec 18 19:18:00 minikube kubelet[77234]: E1218 19:18:00.932969   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-jzwgb" podUID="89fb3918-56a7-4c00-b426-8b66d84531e0"
Dec 18 19:18:02 minikube kubelet[77234]: E1218 19:18:02.933210   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"goprom\" with ImagePullBackOff: \"Back-off pulling image \\\"username/goprom\\\"\"" pod="default/goprom-7bbdc97888-8qgcv" podUID="30f44d01-fe43-40be-846c-e3e78ae1cd93"
Dec 18 19:18:02 minikube kubelet[77234]: E1218 19:18:02.933289   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"goprom\" with ImagePullBackOff: \"Back-off pulling image \\\"username/goprom\\\"\"" pod="default/goprom-7bbdc97888-ktr8q" podUID="36bd20c7-d9e8-48a5-b79e-ee672cb7d714"
Dec 18 19:18:04 minikube kubelet[77234]: E1218 19:18:04.930717   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"goprom\" with ImagePullBackOff: \"Back-off pulling image \\\"username/goprom\\\"\"" pod="default/goprom-7bbdc97888-wmmvl" podUID="3c2f7617-13f9-417e-82bc-427246fe3360"
Dec 18 19:18:05 minikube kubelet[77234]: E1218 19:18:05.930515   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-fv75s" podUID="548b1e30-613d-4b50-bf50-ed293b0cee27"
Dec 18 19:18:07 minikube kubelet[77234]: E1218 19:18:07.934756   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-t6vfm" podUID="7585646c-45f1-490d-aea0-d5f65a1e9ed6"
Dec 18 19:18:09 minikube kubelet[77234]: E1218 19:18:09.674187   77234 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="881a69ac7921557dd4b472eb13be8d81e239816fd7d2e5c91b42a246491db1bb"
Dec 18 19:18:09 minikube kubelet[77234]: E1218 19:18:09.674304   77234 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/monitoring_kube-state-metrics-7589748fd9-sbcpt_3db27fd1-475e-4229-a921-88a928c0f6fc/kube-state-metrics/6.log\": failed to reopen container log \"881a69ac7921557dd4b472eb13be8d81e239816fd7d2e5c91b42a246491db1bb\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/monitoring_kube-state-metrics-7589748fd9-sbcpt_3db27fd1-475e-4229-a921-88a928c0f6fc/kube-state-metrics/6.log" containerID="881a69ac7921557dd4b472eb13be8d81e239816fd7d2e5c91b42a246491db1bb"
Dec 18 19:18:10 minikube kubelet[77234]: E1218 19:18:10.931218   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-qf28v" podUID="228194bd-47d5-4224-9089-1d64fd9be41f"
Dec 18 19:18:12 minikube kubelet[77234]: E1218 19:18:12.932088   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-jzwgb" podUID="89fb3918-56a7-4c00-b426-8b66d84531e0"
Dec 18 19:18:13 minikube kubelet[77234]: E1218 19:18:13.930546   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-bhqv2" podUID="99491fcb-0a27-4f7b-9697-24631450984e"
Dec 18 19:18:14 minikube kubelet[77234]: E1218 19:18:14.931301   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"goprom\" with ImagePullBackOff: \"Back-off pulling image \\\"username/goprom\\\"\"" pod="default/goprom-7bbdc97888-ktr8q" podUID="36bd20c7-d9e8-48a5-b79e-ee672cb7d714"
Dec 18 19:18:18 minikube kubelet[77234]: E1218 19:18:18.930699   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-t6vfm" podUID="7585646c-45f1-490d-aea0-d5f65a1e9ed6"
Dec 18 19:18:18 minikube kubelet[77234]: E1218 19:18:18.930710   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"goprom\" with ImagePullBackOff: \"Back-off pulling image \\\"username/goprom\\\"\"" pod="default/goprom-7bbdc97888-wmmvl" podUID="3c2f7617-13f9-417e-82bc-427246fe3360"
Dec 18 19:18:20 minikube kubelet[77234]: E1218 19:18:20.018970   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-fv75s" podUID="548b1e30-613d-4b50-bf50-ed293b0cee27"
Dec 18 19:18:20 minikube kubelet[77234]: E1218 19:18:20.743280   77234 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="881a69ac7921557dd4b472eb13be8d81e239816fd7d2e5c91b42a246491db1bb"
Dec 18 19:18:20 minikube kubelet[77234]: E1218 19:18:20.743357   77234 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/monitoring_kube-state-metrics-7589748fd9-sbcpt_3db27fd1-475e-4229-a921-88a928c0f6fc/kube-state-metrics/6.log\": failed to reopen container log \"881a69ac7921557dd4b472eb13be8d81e239816fd7d2e5c91b42a246491db1bb\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/monitoring_kube-state-metrics-7589748fd9-sbcpt_3db27fd1-475e-4229-a921-88a928c0f6fc/kube-state-metrics/6.log" containerID="881a69ac7921557dd4b472eb13be8d81e239816fd7d2e5c91b42a246491db1bb"
Dec 18 19:18:21 minikube kubelet[77234]: E1218 19:18:21.522306   77234 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for username/goprom, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="username/goprom:latest"
Dec 18 19:18:21 minikube kubelet[77234]: E1218 19:18:21.522429   77234 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: pull access denied for username/goprom, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="username/goprom:latest"
Dec 18 19:18:21 minikube kubelet[77234]: E1218 19:18:21.522595   77234 kuberuntime_manager.go:1256] container &Container{Name:goprom,Image:username/goprom,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},ContainerPort{Name:probe,HostPort:0,ContainerPort:8086,Protocol:TCP,HostIP:,},ContainerPort{Name:metrics,HostPort:0,ContainerPort:9101,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:VERSION,Value:v1.0.0,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l2289,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/live,Port:{1 0 probe},Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:5,TimeoutSeconds:1,PeriodSeconds:5,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/ready,Port:{1 0 probe},Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:5,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod goprom-7bbdc97888-8qgcv_default(30f44d01-fe43-40be-846c-e3e78ae1cd93): ErrImagePull: Error response from daemon: pull access denied for username/goprom, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Dec 18 19:18:21 minikube kubelet[77234]: E1218 19:18:21.522657   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"goprom\" with ErrImagePull: \"Error response from daemon: pull access denied for username/goprom, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/goprom-7bbdc97888-8qgcv" podUID="30f44d01-fe43-40be-846c-e3e78ae1cd93"
Dec 18 19:18:23 minikube kubelet[77234]: E1218 19:18:23.933722   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-jzwgb" podUID="89fb3918-56a7-4c00-b426-8b66d84531e0"
Dec 18 19:18:23 minikube kubelet[77234]: E1218 19:18:23.934397   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-qf28v" podUID="228194bd-47d5-4224-9089-1d64fd9be41f"
Dec 18 19:18:25 minikube kubelet[77234]: E1218 19:18:25.931339   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"goprom\" with ImagePullBackOff: \"Back-off pulling image \\\"username/goprom\\\"\"" pod="default/goprom-7bbdc97888-ktr8q" podUID="36bd20c7-d9e8-48a5-b79e-ee672cb7d714"
Dec 18 19:18:25 minikube kubelet[77234]: E1218 19:18:25.931355   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-bhqv2" podUID="99491fcb-0a27-4f7b-9697-24631450984e"
Dec 18 19:18:29 minikube kubelet[77234]: E1218 19:18:29.931508   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"goprom\" with ImagePullBackOff: \"Back-off pulling image \\\"username/goprom\\\"\"" pod="default/goprom-7bbdc97888-wmmvl" podUID="3c2f7617-13f9-417e-82bc-427246fe3360"
Dec 18 19:18:31 minikube kubelet[77234]: E1218 19:18:31.419326   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-t6vfm" podUID="7585646c-45f1-490d-aea0-d5f65a1e9ed6"
Dec 18 19:18:31 minikube kubelet[77234]: E1218 19:18:31.839741   77234 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="881a69ac7921557dd4b472eb13be8d81e239816fd7d2e5c91b42a246491db1bb"
Dec 18 19:18:31 minikube kubelet[77234]: E1218 19:18:31.839821   77234 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/monitoring_kube-state-metrics-7589748fd9-sbcpt_3db27fd1-475e-4229-a921-88a928c0f6fc/kube-state-metrics/6.log\": failed to reopen container log \"881a69ac7921557dd4b472eb13be8d81e239816fd7d2e5c91b42a246491db1bb\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/monitoring_kube-state-metrics-7589748fd9-sbcpt_3db27fd1-475e-4229-a921-88a928c0f6fc/kube-state-metrics/6.log" containerID="881a69ac7921557dd4b472eb13be8d81e239816fd7d2e5c91b42a246491db1bb"
Dec 18 19:18:32 minikube kubelet[77234]: E1218 19:18:32.932021   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-fv75s" podUID="548b1e30-613d-4b50-bf50-ed293b0cee27"
Dec 18 19:18:34 minikube kubelet[77234]: E1218 19:18:34.931532   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"goprom\" with ImagePullBackOff: \"Back-off pulling image \\\"username/goprom\\\"\"" pod="default/goprom-7bbdc97888-8qgcv" podUID="30f44d01-fe43-40be-846c-e3e78ae1cd93"
Dec 18 19:18:37 minikube kubelet[77234]: E1218 19:18:37.931284   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-jzwgb" podUID="89fb3918-56a7-4c00-b426-8b66d84531e0"
Dec 18 19:18:38 minikube kubelet[77234]: E1218 19:18:38.930460   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-qf28v" podUID="228194bd-47d5-4224-9089-1d64fd9be41f"
Dec 18 19:18:38 minikube kubelet[77234]: E1218 19:18:38.930481   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-bhqv2" podUID="99491fcb-0a27-4f7b-9697-24631450984e"
Dec 18 19:18:42 minikube kubelet[77234]: E1218 19:18:42.215930   77234 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="881a69ac7921557dd4b472eb13be8d81e239816fd7d2e5c91b42a246491db1bb"
Dec 18 19:18:42 minikube kubelet[77234]: E1218 19:18:42.216010   77234 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/monitoring_kube-state-metrics-7589748fd9-sbcpt_3db27fd1-475e-4229-a921-88a928c0f6fc/kube-state-metrics/6.log\": failed to reopen container log \"881a69ac7921557dd4b472eb13be8d81e239816fd7d2e5c91b42a246491db1bb\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/monitoring_kube-state-metrics-7589748fd9-sbcpt_3db27fd1-475e-4229-a921-88a928c0f6fc/kube-state-metrics/6.log" containerID="881a69ac7921557dd4b472eb13be8d81e239816fd7d2e5c91b42a246491db1bb"
Dec 18 19:18:42 minikube kubelet[77234]: E1218 19:18:42.930584   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"goprom\" with ImagePullBackOff: \"Back-off pulling image \\\"username/goprom\\\"\"" pod="default/goprom-7bbdc97888-wmmvl" podUID="3c2f7617-13f9-417e-82bc-427246fe3360"
Dec 18 19:18:43 minikube kubelet[77234]: E1218 19:18:43.930230   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-t6vfm" podUID="7585646c-45f1-490d-aea0-d5f65a1e9ed6"
Dec 18 19:18:45 minikube kubelet[77234]: E1218 19:18:45.930576   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-fv75s" podUID="548b1e30-613d-4b50-bf50-ed293b0cee27"
Dec 18 19:18:45 minikube kubelet[77234]: E1218 19:18:45.990608   77234 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for username/goprom, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="username/goprom:latest"
Dec 18 19:18:45 minikube kubelet[77234]: E1218 19:18:45.990660   77234 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: pull access denied for username/goprom, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="username/goprom:latest"
Dec 18 19:18:45 minikube kubelet[77234]: E1218 19:18:45.990757   77234 kuberuntime_manager.go:1256] container &Container{Name:goprom,Image:username/goprom,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:http,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},ContainerPort{Name:probe,HostPort:0,ContainerPort:8086,Protocol:TCP,HostIP:,},ContainerPort{Name:metrics,HostPort:0,ContainerPort:9101,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:VERSION,Value:v1.0.0,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c4fkt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/live,Port:{1 0 probe},Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:5,TimeoutSeconds:1,PeriodSeconds:5,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/ready,Port:{1 0 probe},Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:5,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod goprom-7bbdc97888-ktr8q_default(36bd20c7-d9e8-48a5-b79e-ee672cb7d714): ErrImagePull: Error response from daemon: pull access denied for username/goprom, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Dec 18 19:18:45 minikube kubelet[77234]: E1218 19:18:45.990788   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"goprom\" with ErrImagePull: \"Error response from daemon: pull access denied for username/goprom, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/goprom-7bbdc97888-ktr8q" podUID="36bd20c7-d9e8-48a5-b79e-ee672cb7d714"
Dec 18 19:18:46 minikube kubelet[77234]: E1218 19:18:46.931458   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"goprom\" with ImagePullBackOff: \"Back-off pulling image \\\"username/goprom\\\"\"" pod="default/goprom-7bbdc97888-8qgcv" podUID="30f44d01-fe43-40be-846c-e3e78ae1cd93"
Dec 18 19:18:50 minikube kubelet[77234]: E1218 19:18:50.931879   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-qf28v" podUID="228194bd-47d5-4224-9089-1d64fd9be41f"
Dec 18 19:18:51 minikube kubelet[77234]: E1218 19:18:51.929121   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-jzwgb" podUID="89fb3918-56a7-4c00-b426-8b66d84531e0"
Dec 18 19:18:51 minikube kubelet[77234]: E1218 19:18:51.929150   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"devopsprojectmicro\" with ImagePullBackOff: \"Back-off pulling image \\\"houda24/projectdevopsmicro:latest\\\"\"" pod="default/devopsprojectmicro-deployment-866c9d94b9-bhqv2" podUID="99491fcb-0a27-4f7b-9697-24631450984e"
Dec 18 19:18:53 minikube kubelet[77234]: E1218 19:18:53.610527   77234 remote_runtime.go:753] "ReopenContainerLog from runtime service failed" err="rpc error: code = Unknown desc = docker does not support reopening container log files" containerID="881a69ac7921557dd4b472eb13be8d81e239816fd7d2e5c91b42a246491db1bb"
Dec 18 19:18:53 minikube kubelet[77234]: E1218 19:18:53.610619   77234 container_log_manager.go:268] "Failed to rotate log for container" err="failed to rotate log \"/var/log/pods/monitoring_kube-state-metrics-7589748fd9-sbcpt_3db27fd1-475e-4229-a921-88a928c0f6fc/kube-state-metrics/6.log\": failed to reopen container log \"881a69ac7921557dd4b472eb13be8d81e239816fd7d2e5c91b42a246491db1bb\": rpc error: code = Unknown desc = docker does not support reopening container log files" path="/var/log/pods/monitoring_kube-state-metrics-7589748fd9-sbcpt_3db27fd1-475e-4229-a921-88a928c0f6fc/kube-state-metrics/6.log" containerID="881a69ac7921557dd4b472eb13be8d81e239816fd7d2e5c91b42a246491db1bb"
Dec 18 19:18:54 minikube kubelet[77234]: E1218 19:18:54.018850   77234 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"goprom\" with ImagePullBackOff: \"Back-off pulling image \\\"username/goprom\\\"\"" pod="default/goprom-7bbdc97888-wmmvl" podUID="3c2f7617-13f9-417e-82bc-427246fe3360"

* 
* ==> storage-provisioner [135bf9fcff6f] <==
* I1218 15:56:01.011989       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1218 15:56:08.002329       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1218 15:56:08.002499       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1218 15:56:25.603990       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1218 15:56:25.604131       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_0a0decb3-3f2e-4135-bc96-0ad6dd60e612!
I1218 15:56:25.604181       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"cb26a841-f424-4bc1-bc89-4a317e0c235a", APIVersion:"v1", ResourceVersion:"23445", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_0a0decb3-3f2e-4135-bc96-0ad6dd60e612 became leader
I1218 15:56:25.705285       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_0a0decb3-3f2e-4135-bc96-0ad6dd60e612!

* 
* ==> storage-provisioner [ccb66d4c9981] <==
* I1218 14:15:04.513728       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1218 14:15:04.527940       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1218 14:15:04.528038       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1218 14:15:22.525475       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1218 14:15:22.528193       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_afdcb47a-1206-4bf2-802c-a4bb42cb654e!
I1218 14:15:22.714956       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"cb26a841-f424-4bc1-bc89-4a317e0c235a", APIVersion:"v1", ResourceVersion:"17936", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_afdcb47a-1206-4bf2-802c-a4bb42cb654e became leader
I1218 14:15:22.832290       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_afdcb47a-1206-4bf2-802c-a4bb42cb654e!

